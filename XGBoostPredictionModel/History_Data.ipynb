{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45316cf1-79e3-4f46-8dc0-3c2698346769",
   "metadata": {},
   "source": [
    "# Get Raw data from www.champsorchump.us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98e512c5-72a5-44f4-b1f8-7431c57b377c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter 'all' for all teams or specify a team (e.g., 'toronto-raptors'):  all\n",
      "Enter your start year:  2023\n",
      "Enter your end year:  2023\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nba_teams' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m master_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m team_selection\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m team \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnba_teams\u001b[49m:\n\u001b[0;32m     51\u001b[0m         df \u001b[38;5;241m=\u001b[39m scrape_data_for_team(team, start_year, end_year)\n\u001b[0;32m     52\u001b[0m         master_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([master_df, df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nba_teams' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def process_game_row(tr, team):\n",
    "    date = tr.find('td', class_='text-center').get_text(strip=True)\n",
    "    opponent_td = tr.find_all('td', class_='text-center')[1]\n",
    "    opponent_raw = opponent_td.get_text(strip=True)\n",
    "    score = tr.find_all('td', class_='text-center')[2].get_text(strip=True)\n",
    "    is_playoff = opponent_td.find('span', class_='fa fa-bolt') is not None\n",
    "    type = \"Playoff\" if is_playoff else \"Regular\"\n",
    "    win_loss = \"Win\" if 'W' in score else \"Loss\"\n",
    "    team1 = team.replace('-', '')\n",
    "    opponent = opponent_raw.replace('@ ', '').replace('vs ', '').replace('-', '')\n",
    "    return {'Date': date, 'Score': score, 'Win/Loss': win_loss, 'Type': type, 'Team1': team1, 'Team2': opponent}\n",
    "\n",
    "def scrape_data_for_year(team, year):\n",
    "    url = f\"https://champsorchumps.us/team/nba/{team}/{year}\"\n",
    "    response = requests.get(url)\n",
    "    print(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        target_div = soup.find('div', class_='col-md-12 col-lg-8')\n",
    "        if target_div:\n",
    "            game_rows = target_div.find_all('tr', id=lambda x: x and x.startswith('game_'))\n",
    "            return [process_game_row(tr, team) for tr in game_rows if tr.find('td', class_='text-center')]\n",
    "        else:\n",
    "            print(f\"Element not found for year {year}.\")\n",
    "            return []\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the web page for year {year}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def scrape_data_for_team(team, start_year, end_year):\n",
    "    all_games_data = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        yearly_data = scrape_data_for_year(team, year)\n",
    "        all_games_data.extend(yearly_data)\n",
    "        time.sleep(1) # delay in seconds \n",
    "    return pd.DataFrame(all_games_data)\n",
    "\n",
    "team_selection = input(\"Enter 'all' for all teams or specify a team (e.g., 'toronto-raptors'): \").strip()\n",
    "start_year = int(input(\"Enter your start year: \"))\n",
    "end_year = int(input(\"Enter your end year: \"))\n",
    "\n",
    "master_df = pd.DataFrame()\n",
    "\n",
    "if team_selection.lower() == 'all':\n",
    "    for team in nba_teams:\n",
    "        df = scrape_data_for_team(team, start_year, end_year)\n",
    "        master_df = pd.concat([master_df, df], ignore_index=True)\n",
    "else:\n",
    "    master_df = scrape_data_for_team(team_selection, start_year, end_year)\n",
    "\n",
    "# Rename 'Win/Loss' column to 'Win'\n",
    "master_df.rename(columns={'Win/Loss': 'Win'}, inplace=True)\n",
    "\n",
    "# Convert 'Type' column: 'Playoff' to 1, 'Regular' to 0\n",
    "master_df['Type'] = master_df['Type'].apply(lambda x: 1 if x == 'Playoff' else 0)\n",
    "\n",
    "master_df['Win'] = master_df['Win'].apply(lambda x: 1 if x == 'Win' else 0)\n",
    "\n",
    "# Check and remove rows where 'Score' column is empty\n",
    "master_df = master_df[master_df['Score'].str.strip().ne('')]\n",
    "\n",
    "csv_file_name = f\"nba_data_{start_year}_to_{end_year}.csv\"\n",
    "master_df.to_csv(csv_file_name, index=False)\n",
    "print(f\"CSV file generated: {csv_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da832303-c17c-477a-b76c-b121259510ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6196c898-953a-46f8-a831-1121d0da20f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd018247-c030-4dd1-8eb9-00fddb94e3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter 'all' for all teams or specify a team (e.g., 'toronto-raptors'):  all\n",
      "Enter your start year:  2023\n",
      "Enter your end year:  2023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for atlanta-hawks...\n",
      "Scraping data for boston-celtics...\n",
      "Scraping data for brooklyn-nets...\n",
      "Scraping data for charlotte-hornets...\n",
      "Scraping data for chicago-bulls...\n",
      "Scraping data for cleveland-cavaliers...\n",
      "Scraping data for dallas-mavericks...\n",
      "Scraping data for denver-nuggets...\n",
      "Scraping data for detroit-pistons...\n",
      "Scraping data for golden-state-warriors...\n",
      "Scraping data for houston-rockets...\n",
      "Scraping data for indiana-pacers...\n",
      "Scraping data for los-angeles-clippers...\n",
      "Scraping data for los-angeles-lakers...\n",
      "Scraping data for memphis-grizzlies...\n",
      "Scraping data for miami-heat...\n",
      "Scraping data for milwaukee-bucks...\n",
      "Scraping data for minnesota-timberwolves...\n",
      "Scraping data for new-orleans-pelicans...\n",
      "Scraping data for new-york-knicks...\n",
      "Scraping data for oklahoma-city-thunder...\n",
      "Scraping data for orlando-magic...\n",
      "Scraping data for philadelphia-76ers...\n",
      "Scraping data for phoenix-suns...\n",
      "Scraping data for portland-trail-blazers...\n",
      "Scraping data for sacramento-kings...\n",
      "Scraping data for san-antonio-spurs...\n",
      "Scraping data for toronto-raptors...\n",
      "Scraping data for utah-jazz...\n",
      "Scraping data for washington-wizards...\n",
      "                Date         Score  Win  Type          Team1  \\\n",
      "0  Wed, Oct 19, 2022   Win117- 107    1     0  atlanta hawks   \n",
      "1  Fri, Oct 21, 2022    Win108- 98    1     0  atlanta hawks   \n",
      "2  Sun, Oct 23, 2022  Loss109 -126    0     0  atlanta hawks   \n",
      "3  Wed, Oct 26, 2022   Win118- 113    1     0  atlanta hawks   \n",
      "4  Fri, Oct 28, 2022   Win136- 112    1     0  atlanta hawks   \n",
      "\n",
      "                Team2  OT  \n",
      "0    vsHoustonRockets   0  \n",
      "1      vsOrlandoMagic   0  \n",
      "2  vsCharlotteHornets   0  \n",
      "3     @DetroitPistons   0  \n",
      "4     @DetroitPistons   0  \n",
      "CSV file generated: nba_data_2023_to_2023.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define the NBA teams\n",
    "nba_teams = [\n",
    "    'atlanta-hawks', 'boston-celtics', 'brooklyn-nets', 'charlotte-hornets',\n",
    "    'chicago-bulls', 'cleveland-cavaliers', 'dallas-mavericks', 'denver-nuggets',\n",
    "    'detroit-pistons', 'golden-state-warriors', 'houston-rockets', 'indiana-pacers',\n",
    "    'los-angeles-clippers', 'los-angeles-lakers', 'memphis-grizzlies', 'miami-heat',\n",
    "    'milwaukee-bucks', 'minnesota-timberwolves', 'new-orleans-pelicans', 'new-york-knicks',\n",
    "    'oklahoma-city-thunder', 'orlando-magic', 'philadelphia-76ers', 'phoenix-suns',\n",
    "    'portland-trail-blazers', 'sacramento-kings', 'san-antonio-spurs', 'toronto-raptors',\n",
    "    'utah-jazz', 'washington-wizards'\n",
    "]\n",
    "\n",
    "def process_game_row(tr, team):\n",
    "    date = tr.find('td', class_='text-center').get_text(strip=True)\n",
    "    opponent_td = tr.find_all('td', class_='text-center')[1]\n",
    "    opponent_raw = opponent_td.get_text(strip=True)\n",
    "    score = tr.find_all('td', class_='text-center')[2].get_text(strip=True)\n",
    "    is_playoff = opponent_td.find('span', class_='fa fa-bolt') is not None\n",
    "    type = \"Playoff\" if is_playoff else \"Regular\"\n",
    "    win_loss = \"Win\" if 'W' in score else \"Loss\"\n",
    "    team1 = team.replace('-', ' ')\n",
    "    opponent = opponent_raw.replace('@ ', '').replace('vs ', '').replace('-', ' ')\n",
    "    \n",
    "    # Check for OT by looking for a <td> with specific classes and text starting with \"OT\"\n",
    "    ot_indicator = tr.find('td', class_='expanding text-center team-lost  pl-1 pr-1')\n",
    "    OT = 1 if ot_indicator and ot_indicator.get_text(strip=True).startswith(\"OT\") else 0\n",
    "    \n",
    "    return {'Date': date, 'Score': score, 'Win': win_loss, 'Type': type, 'Team1': team1, 'Team2': opponent, 'OT': OT}\n",
    "\n",
    "def scrape_data_for_year(team, year):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    url = f\"https://champsorchumps.us/team/nba/{team}/{year}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        target_div = soup.find('div', class_='col-md-12 col-lg-8')\n",
    "        if target_div:\n",
    "            game_rows = target_div.find_all('tr', id=lambda x: x and x.startswith('game_'))\n",
    "            return [process_game_row(tr, team) for tr in game_rows if tr.find('td', class_='text-center')]\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the web page for year {year}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def scrape_data_for_team(team, start_year, end_year):\n",
    "    all_games_data = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        yearly_data = scrape_data_for_year(team, year)\n",
    "        all_games_data.extend(yearly_data)\n",
    "        time.sleep(1)\n",
    "    return pd.DataFrame(all_games_data)\n",
    "\n",
    "# Main script to handle user inputs and aggregate data\n",
    "team_selection = input(\"Enter 'all' for all teams or specify a team (e.g., 'toronto-raptors'): \").strip().lower().replace(' ', '-')\n",
    "start_year = int(input(\"Enter your start year: \"))\n",
    "end_year = int(input(\"Enter your end year: \"))\n",
    "\n",
    "master_df = pd.DataFrame()\n",
    "\n",
    "if team_selection == 'all':\n",
    "    for team in nba_teams:\n",
    "        print(f\"Scraping data for {team}...\")\n",
    "        df = scrape_data_for_team(team, start_year, end_year)\n",
    "        master_df = pd.concat([master_df, df], ignore_index=True)\n",
    "else:\n",
    "    master_df = scrape_data_for_team(team_selection, start_year, end_year)\n",
    "\n",
    "# Data post-processing\n",
    "master_df['Win'] = master_df['Win'].apply(lambda x: 1 if x == 'Win' else 0)\n",
    "master_df['Type'] = master_df['Type'].apply(lambda x: 1 if x == 'Playoff' else 0)\n",
    "\n",
    "print(master_df.head())\n",
    "\n",
    "# Export to CSV\n",
    "csv_file_name = f\"nba_data_{start_year}_to_{end_year}.csv\"\n",
    "master_df.to_csv(csv_file_name, index=False)\n",
    "print(f\"CSV file generated: {csv_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e297a4-038b-4cf8-841a-3aaa323fb7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a5f9b4-f4bc-4361-8ed3-f6d7d34ea32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET OVERTIME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63c33acb-188d-4cbb-b7c3-e5661040e01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OT Games Found with Dates:\n",
      "Wed, Dec  7, 2022\n",
      "Mon, Dec 26, 2022\n",
      "Sat, Feb  4, 2023\n",
      "Fri, Feb 24, 2023\n",
      "Sun, Feb 26, 2023\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def download_and_parse_html(url):\n",
    "    # Use a common user-agent to mimic a browser request\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f\"Error fetching the page: Status code {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def find_ot_games(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    ot_games = []  # Initialize an empty list to hold games that went into overtime\n",
    "\n",
    "    # Find all rows in the schedule table\n",
    "    for game_row in soup.find_all('tr'):\n",
    "        date_cell = game_row.find('td', class_='text-center')  # Find the cell with the game date\n",
    "        expanding_td = game_row.find('td', class_='expanding')  # Look for the expanding <td> which might indicate more details\n",
    "        \n",
    "        # Check if this row represents an OT game by looking for \"OT\" in the expanding <td>\n",
    "        if expanding_td and 'OT' in expanding_td.text:\n",
    "            if date_cell:\n",
    "                date_text = date_cell.text.strip()  # Extract the game date\n",
    "                ot_games.append(date_text)  # Add the date of the OT game to the list\n",
    "\n",
    "    return ot_games\n",
    "\n",
    "# URL of the Los Angeles Clippers' 2023 schedule\n",
    "url = \"https://champsorchumps.us/team/nba/los-angeles-clippers/2023\"\n",
    "\n",
    "# Download and parse the HTML content\n",
    "html_content = download_and_parse_html(url)\n",
    "\n",
    "if html_content:\n",
    "    # Find and list all OT games with their dates\n",
    "    ot_games = find_ot_games(html_content)\n",
    "    if ot_games:\n",
    "        print(\"OT Games Found with Dates:\")\n",
    "        for game in ot_games:\n",
    "            print(game)\n",
    "    else:\n",
    "        print(\"No OT games found.\")\n",
    "else:\n",
    "    print(\"Failed to download or parse the page.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11700c88-393f-44b0-a2e7-18d25e488284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007cf139-7703-4ec4-8842-6abbfff99e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f996612-0e03-483f-86eb-7fbfa9f081eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b9e49a3-0b67-4f5a-8dc9-259bca78ee16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter 'all' for all teams or specify a team (e.g., 'los-angeles-clippers'):  toronto-raptors\n",
      "Enter your start year:  2024\n",
      "Enter your end year:  2024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for toronto-raptors, year: 2024\n",
      "CSV file generated: nba_data_2024_to_2024.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define the NBA teams\n",
    "nba_teams = [\n",
    "    'atlanta-hawks', 'boston-celtics', 'brooklyn-nets', 'charlotte-hornets',\n",
    "    'chicago-bulls', 'cleveland-cavaliers', 'dallas-mavericks', 'denver-nuggets',\n",
    "    'detroit-pistons', 'golden-state-warriors', 'houston-rockets', 'indiana-pacers',\n",
    "    'los-angeles-clippers', 'los-angeles-lakers', 'memphis-grizzlies', 'miami-heat',\n",
    "    'milwaukee-bucks', 'minnesota-timberwolves', 'new-orleans-pelicans', 'new-york-knicks',\n",
    "    'oklahoma-city-thunder', 'orlando-magic', 'philadelphia-76ers', 'phoenix-suns',\n",
    "    'portland-trail-blazers', 'sacramento-kings', 'san-antonio-spurs', 'toronto-raptors',\n",
    "    'utah-jazz', 'washington-wizards'\n",
    "]\n",
    "\n",
    "def process_game_row(tr, team):\n",
    "    date = tr.find('td', class_='text-center').get_text(strip=True)\n",
    "    opponent_td = tr.find_all('td', class_='text-center')[1]\n",
    "    opponent_raw = opponent_td.get_text(strip=True)\n",
    "    score = tr.find_all('td', class_='text-center')[2].get_text(strip=True)\n",
    "    is_playoff = opponent_td.find('span', class_='fa fa-bolt') is not None\n",
    "    type = \"Playoff\" if is_playoff else \"Regular\"\n",
    "    win_loss = \"Win\" if 'W' in score else \"Loss\"\n",
    "    team1 = team.replace('-', ' ')\n",
    "    opponent = opponent_raw.replace('@ ', '').replace('vs ', '').replace('-', ' ')\n",
    "    \n",
    "    # Implement OT detection based on the provided method\n",
    "    ot_game = 0  # Default to no OT\n",
    "    expanding_td = tr.find('td', class_='expanding')\n",
    "    if expanding_td and 'OT' in expanding_td.text:\n",
    "        ot_game = 1  # Mark as OT game\n",
    "\n",
    "    return {\n",
    "        'Date': date, 'Score': score, 'Win': win_loss, 'Type': type,\n",
    "        'Team1': team1, 'Team2': opponent, 'OT': ot_game\n",
    "    }\n",
    "\n",
    "def scrape_data_for_year(team, year):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    url = f\"https://champsorchumps.us/team/nba/{team}/{year}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        target_div = soup.find('div', class_='col-md-12 col-lg-8')\n",
    "        if target_div:\n",
    "            game_rows = target_div.find_all('tr', id=lambda x: x and x.startswith('game_'))\n",
    "            return [process_game_row(tr, team) for tr in game_rows if tr.find('td', class_='text-center')]\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the web page for year {year}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def scrape_data_for_team(team, start_year, end_year):\n",
    "    all_games_data = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        print(f\"Scraping data for {team}, year: {year}\")\n",
    "        yearly_data = scrape_data_for_year(team, year)\n",
    "        all_games_data.extend(yearly_data)\n",
    "        time.sleep(1)  # Respectful delay to avoid server overload\n",
    "    return pd.DataFrame(all_games_data)\n",
    "\n",
    "team_selection = input(\"Enter 'all' for all teams or specify a team (e.g., 'los-angeles-clippers'): \").strip().lower().replace(' ', '-')\n",
    "start_year = int(input(\"Enter your start year: \"))\n",
    "end_year = int(input(\"Enter your end year: \"))\n",
    "\n",
    "master_df = pd.DataFrame()\n",
    "\n",
    "if team_selection == 'all':\n",
    "    for team in nba_teams:\n",
    "        df = scrape_data_for_team(team, start_year, end_year)\n",
    "        master_df = pd.concat([master_df, df], ignore_index=True)\n",
    "else:\n",
    "    if team_selection in nba_teams:\n",
    "        master_df = scrape_data_for_team(team_selection, start_year, end_year)\n",
    "    else:\n",
    "        print(\"Team not found. Please ensure you've entered the team name correctly.\")\n",
    "\n",
    "# Data post-processing\n",
    "master_df['Win'] = master_df['Win'].apply(lambda x: 1 if x == 'Win' else 0)\n",
    "master_df['Type'] = master_df['Type'].apply(lambda x: 1 if x == 'Playoff' else 0)\n",
    "\n",
    "# Export to CSV\n",
    "csv_file_name = f\"nba_data_{start_year}_to_{end_year}.csv\"\n",
    "master_df.to_csv(csv_file_name, index=False)\n",
    "print(f\"CSV file generated: {csv_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d412db9-7f67-4733-b9cb-aea8c1c70691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9d837f-0c65-42f3-870b-9b481fcb563c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a1f2303-7256-4077-bf03-cae1ec90f8d7",
   "metadata": {},
   "source": [
    "# WORK! Use this for scraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5e8514b0-553d-406b-a766-3eeee022fec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter 'all' for all teams or specify a team (e.g., 'los-angeles-clippers'):  all\n",
      "Enter your start year:  2023\n",
      "Enter your end year:  2024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for atlanta-hawks. Year 2023.\n",
      "Scraping data for atlanta-hawks. Year 2024.\n",
      "Scraping data for boston-celtics. Year 2023.\n",
      "Scraping data for boston-celtics. Year 2024.\n",
      "Scraping data for brooklyn-nets. Year 2023.\n",
      "Scraping data for brooklyn-nets. Year 2024.\n",
      "Scraping data for charlotte-hornets. Year 2023.\n",
      "Scraping data for charlotte-hornets. Year 2024.\n",
      "Scraping data for chicago-bulls. Year 2023.\n",
      "Scraping data for chicago-bulls. Year 2024.\n",
      "Scraping data for cleveland-cavaliers. Year 2023.\n",
      "Scraping data for cleveland-cavaliers. Year 2024.\n",
      "Scraping data for dallas-mavericks. Year 2023.\n",
      "Scraping data for dallas-mavericks. Year 2024.\n",
      "Scraping data for denver-nuggets. Year 2023.\n",
      "Scraping data for denver-nuggets. Year 2024.\n",
      "Scraping data for detroit-pistons. Year 2023.\n",
      "Scraping data for detroit-pistons. Year 2024.\n",
      "Scraping data for golden-state-warriors. Year 2023.\n",
      "Scraping data for golden-state-warriors. Year 2024.\n",
      "Scraping data for houston-rockets. Year 2023.\n",
      "Scraping data for houston-rockets. Year 2024.\n",
      "Scraping data for indiana-pacers. Year 2023.\n",
      "Scraping data for indiana-pacers. Year 2024.\n",
      "Scraping data for los-angeles-clippers. Year 2023.\n",
      "Scraping data for los-angeles-clippers. Year 2024.\n",
      "Scraping data for los-angeles-lakers. Year 2023.\n",
      "Scraping data for los-angeles-lakers. Year 2024.\n",
      "Scraping data for memphis-grizzlies. Year 2023.\n",
      "Scraping data for memphis-grizzlies. Year 2024.\n",
      "Scraping data for miami-heat. Year 2023.\n",
      "Scraping data for miami-heat. Year 2024.\n",
      "Scraping data for milwaukee-bucks. Year 2023.\n",
      "Scraping data for milwaukee-bucks. Year 2024.\n",
      "Scraping data for minnesota-timberwolves. Year 2023.\n",
      "Scraping data for minnesota-timberwolves. Year 2024.\n",
      "Scraping data for new-orleans-pelicans. Year 2023.\n",
      "Scraping data for new-orleans-pelicans. Year 2024.\n",
      "Scraping data for new-york-knicks. Year 2023.\n",
      "Scraping data for new-york-knicks. Year 2024.\n",
      "Scraping data for oklahoma-city-thunder. Year 2023.\n",
      "Scraping data for oklahoma-city-thunder. Year 2024.\n",
      "Scraping data for orlando-magic. Year 2023.\n",
      "Scraping data for orlando-magic. Year 2024.\n",
      "Scraping data for philadelphia-76ers. Year 2023.\n",
      "Scraping data for philadelphia-76ers. Year 2024.\n",
      "Scraping data for phoenix-suns. Year 2023.\n",
      "Scraping data for phoenix-suns. Year 2024.\n",
      "Scraping data for portland-trail-blazers. Year 2023.\n",
      "Scraping data for portland-trail-blazers. Year 2024.\n",
      "Scraping data for sacramento-kings. Year 2023.\n",
      "Scraping data for sacramento-kings. Year 2024.\n",
      "Scraping data for san-antonio-spurs. Year 2023.\n",
      "Scraping data for san-antonio-spurs. Year 2024.\n",
      "Scraping data for toronto-raptors. Year 2023.\n",
      "Scraping data for toronto-raptors. Year 2024.\n",
      "Scraping data for utah-jazz. Year 2023.\n",
      "Scraping data for utah-jazz. Year 2024.\n",
      "Scraping data for washington-wizards. Year 2023.\n",
      "Scraping data for washington-wizards. Year 2024.\n",
      "CSV file generated: nba_data_2023_to_2024.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define the NBA teams\n",
    "nba_teams = [\n",
    "    'atlanta-hawks', 'boston-celtics', 'brooklyn-nets', 'charlotte-hornets',\n",
    "    'chicago-bulls', 'cleveland-cavaliers', 'dallas-mavericks', 'denver-nuggets',\n",
    "    'detroit-pistons', 'golden-state-warriors', 'houston-rockets', 'indiana-pacers',\n",
    "    'los-angeles-clippers', 'los-angeles-lakers', 'memphis-grizzlies', 'miami-heat',\n",
    "    'milwaukee-bucks', 'minnesota-timberwolves', 'new-orleans-pelicans', 'new-york-knicks',\n",
    "    'oklahoma-city-thunder', 'orlando-magic', 'philadelphia-76ers', 'phoenix-suns',\n",
    "    'portland-trail-blazers', 'sacramento-kings', 'san-antonio-spurs', 'toronto-raptors',\n",
    "    'utah-jazz', 'washington-wizards'\n",
    "]\n",
    "\n",
    "def process_game_row(tr, team):\n",
    "    date = tr.find('td', class_='text-center').get_text(strip=True)\n",
    "    opponent_td = tr.find_all('td', class_='text-center')[1]\n",
    "    opponent_raw = opponent_td.get_text(strip=True)\n",
    "    score = tr.find_all('td', class_='text-center')[2].get_text(strip=True)\n",
    "    is_playoff = opponent_td.find('span', class_='fa fa-bolt') is not None\n",
    "    type = \"Playoff\" if is_playoff else \"Regular\"\n",
    "    win_loss = \"Win\" if 'W' in score else \"Loss\"\n",
    "    team1 = team.replace('-', ' ')\n",
    "    opponent = opponent_raw.replace('@ ', '').replace('vs ', '').replace('-', ' ')\n",
    "    \n",
    "    ot_game = 0  # Default to no OT\n",
    "    expanding_td = tr.find('td', class_='expanding')\n",
    "    if expanding_td and 'OT' in expanding_td.text:\n",
    "        ot_game = 1  # Mark as OT game\n",
    "\n",
    "    return {\n",
    "        'Date': date, 'Score': score, 'Win': win_loss, 'Type': type,\n",
    "        'Team1': team1, 'Team2': opponent, 'OT': ot_game\n",
    "    }\n",
    "\n",
    "def scrape_data_for_year(team, year):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    url = f\"https://champsorchumps.us/team/nba/{team}/{year}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        target_div = soup.find('div', class_='col-md-12 col-lg-8')\n",
    "        if target_div:\n",
    "            game_rows = target_div.find_all('tr', id=lambda x: x and x.startswith('game_'))\n",
    "            print(f\"Scraping data for {team}. Year {year}.\")\n",
    "            return [process_game_row(tr, team) for tr in game_rows if tr.find('td', class_='text-center')]\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the web page for year {year}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def scrape_data_for_team(team, start_year, end_year):\n",
    "    all_games_data = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        yearly_data = scrape_data_for_year(team, year)\n",
    "        all_games_data.extend(yearly_data)\n",
    "        time.sleep(1)  # Delay\n",
    "    return pd.DataFrame(all_games_data)\n",
    "\n",
    "team_selection = input(\"Enter 'all' for all teams or specify a team (e.g., 'los-angeles-clippers'): \").strip().lower().replace(' ', '-')\n",
    "start_year = int(input(\"Enter your start year: \"))\n",
    "end_year = int(input(\"Enter your end year: \"))\n",
    "\n",
    "master_df = pd.DataFrame()\n",
    "\n",
    "if team_selection == 'all':\n",
    "    for team in nba_teams:\n",
    "        df = scrape_data_for_team(team, start_year, end_year)\n",
    "        master_df = pd.concat([master_df, df], ignore_index=True)\n",
    "else:\n",
    "    if team_selection in nba_teams:\n",
    "        master_df = scrape_data_for_team(team_selection, start_year, end_year)\n",
    "    else:\n",
    "        print(\"Team not found. Please ensure you've entered the team name correctly.\")\n",
    "\n",
    "# Remove rows with an empty 'Score' column\n",
    "master_df = master_df[master_df['Score'].str.strip() != '']\n",
    "\n",
    "# Data post-processing\n",
    "master_df['Win'] = master_df['Win'].apply(lambda x: 1 if x == 'Win' else 0)\n",
    "master_df['Type'] = master_df['Type'].apply(lambda x: 1 if x == 'Playoff' else 0)\n",
    "\n",
    "# Export to CSV\n",
    "csv_file_name = f\"nba_data_{start_year}_to_{end_year}.csv\"\n",
    "master_df.to_csv(csv_file_name, index=False)\n",
    "print(f\"CSV file generated: {csv_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d787ce2-ec04-4451-b729-9e72d632a504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f797e507-8996-4908-88d0-0130aaac4c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c71c55-4a4f-4099-b784-58769e81307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST ADDITIONAL FEATURES  DO NOT USE!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "714ca726-7170-45ea-95e8-1ddc6fe523dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date  Win  Type               Team1               Team2  Team1_Score  \\\n",
      "0 2014-10-28    1     1     SanAntonioSpurs     DallasMavericks        101.0   \n",
      "1 2014-10-28    0     1     DallasMavericks     SanAntonioSpurs        100.0   \n",
      "2 2014-10-28    0     1        OrlandoMagic  NewOrleansPelicans         84.0   \n",
      "3 2014-10-28    1     0  NewOrleansPelicans        OrlandoMagic        101.0   \n",
      "4 2014-10-28    1     0      HoustonRockets    LosAngelesLakers        108.0   \n",
      "\n",
      "   Team2_Score   Avg_Score  Win_Ratio                          Matchup  \\\n",
      "0        100.0  108.470443   0.529557  DallasMavericks_SanAntonioSpurs   \n",
      "1        101.0  108.038272   0.492593  DallasMavericks_SanAntonioSpurs   \n",
      "2        101.0  104.425997   0.377091  NewOrleansPelicans_OrlandoMagic   \n",
      "3         84.0  109.980964   0.463198  NewOrleansPelicans_OrlandoMagic   \n",
      "4         90.0  110.674224   0.516706  HoustonRockets_LosAngelesLakers   \n",
      "\n",
      "   Team1_H2H_Wins  Team2_H2H_Wins  H2H_Matchups  H2H_Win_Ratio  \n",
      "0             1.0             1.0           2.0            0.5  \n",
      "1             1.0             1.0           2.0            0.5  \n",
      "2             1.0             1.0           2.0            0.5  \n",
      "3             1.0             1.0           2.0            0.5  \n",
      "4             1.0             0.0           1.0            0.0  \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to save the modified DataFrame to a CSV file? (yes/no):  YES\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved as 'nba_games_with_corrected_h2h_features.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame creation\n",
    "data = {\n",
    "    'Date': [1414454400, 1414454400, 1414454400, 1414454400, 1414454400],\n",
    "    'Win': [1, 0, 0, 1, 1],\n",
    "    'Type': [1, 1, 1, 0, 0],\n",
    "    'Team1': ['SanAntonioSpurs', 'DallasMavericks', 'OrlandoMagic', 'NewOrleansPelicans', 'HoustonRockets'],\n",
    "    'Team2': ['DallasMavericks', 'SanAntonioSpurs', 'NewOrleansPelicans', 'OrlandoMagic', 'LosAngelesLakers'],\n",
    "    'Team1_Score': [101.0, 100.0, 84.0, 101.0, 108.0],\n",
    "    'Team2_Score': [100.0, 101.0, 101.0, 84.0, 90.0],\n",
    "    'Avg_Score': [108.470443, 108.038272, 104.425997, 109.980964, 110.674224],\n",
    "    'Win_Ratio': [0.529557, 0.492593, 0.377091, 0.463198, 0.516706]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert the date from UNIX timestamp to datetime for sorting\n",
    "df['Date'] = pd.to_datetime(df['Date'], unit='s')\n",
    "\n",
    "# Create a unique identifier for each game ignoring who is home/away\n",
    "df['Matchup'] = df.apply(lambda x: '_'.join(sorted([x['Team1'], x['Team2']])), axis=1)\n",
    "\n",
    "# Create a dictionary to store head-to-head wins\n",
    "h2h_wins = {}\n",
    "\n",
    "# Iterate through each game to populate the dictionary\n",
    "for i, row in df.iterrows():\n",
    "    matchup = row['Matchup']\n",
    "    winner = row['Team1'] if row['Win'] == 1 else row['Team2']\n",
    "    if matchup not in h2h_wins:\n",
    "        h2h_wins[matchup] = {'Team1': 0, 'Team2': 0}\n",
    "    # Increment the win count for the winning team in this matchup\n",
    "    if winner == row['Team1']:\n",
    "        h2h_wins[matchup]['Team1'] += 1\n",
    "    else:\n",
    "        h2h_wins[matchup]['Team2'] += 1\n",
    "\n",
    "# Apply the accumulated head-to-head stats to the DataFrame\n",
    "for i, row in df.iterrows():\n",
    "    matchup = row['Matchup']\n",
    "    df.at[i, 'Team1_H2H_Wins'] = h2h_wins[matchup]['Team1']\n",
    "    df.at[i, 'Team2_H2H_Wins'] = h2h_wins[matchup]['Team2']\n",
    "    df.at[i, 'H2H_Matchups'] = h2h_wins[matchup]['Team1'] + h2h_wins[matchup]['Team2']\n",
    "    if df.at[i, 'H2H_Matchups'] > 0:\n",
    "        # Calculate the win ratio based on which team is listed as Team1 in the current row\n",
    "        if row['Team1'] in h2h_wins[matchup]:\n",
    "            df.at[i, 'H2H_Win_Ratio'] = h2h_wins[matchup]['Team1'] / df.at[i, 'H2H_Matchups']\n",
    "        else:\n",
    "            df.at[i, 'H2H_Win_Ratio'] = h2h_wins[matchup]['Team2'] / df.at[i, 'H2H_Matchups']\n",
    "\n",
    "# Now display the adjusted DataFrame head to verify the calculation\n",
    "print(df.head())\n",
    "\n",
    "# Ask user to save the file\n",
    "save_file = input(\"Do you want to save the modified DataFrame to a CSV file? (yes/no): \").strip().lower()\n",
    "if save_file == 'yes':\n",
    "    df.to_csv('nba_games_with_corrected_h2h_features.csv', index=False)\n",
    "    print(\"DataFrame saved as 'nba_games_with_corrected_h2h_features.csv'.\")\n",
    "else:\n",
    "    print(\"DataFrame not saved.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c896f4-4fe1-4228-97fb-5781c47c13eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db52ca53-0802-4923-8ba7-d1881725aa71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711b341d-fc98-4dc1-b45a-d89e65e8944b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde39a21-b88a-4e3e-b2ff-16d9a5e30323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10862ba-799f-46ed-b007-db6bcecbd6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dbcab1-312e-4d9e-99dc-251cb1506290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe50b1b0-0f5d-4414-afcb-0f6e6c3df949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a033370-d3f0-4b85-afb5-018c0ce002d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f02418-db51-4797-9700-c181a980b41d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b2b866d5-2390-45ef-8589-7f58cf111bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to nba_scores.csv.\n",
      "Temporary HTML file deleted: C:\\Users\\Tommy\\AppData\\Local\\Temp\\tmphvt_hoa4.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "def download_html(url):\n",
    "    \"\"\"Download HTML content of a page and save it to a temporary file.\"\"\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        temp_file = tempfile.NamedTemporaryFile(delete=False, mode='w+', encoding='utf-8', suffix='.html')\n",
    "        temp_file.write(response.text)\n",
    "        temp_file.close()\n",
    "        return temp_file.name\n",
    "    else:\n",
    "        print(\"Failed to retrieve the page.\")\n",
    "        return None\n",
    "\n",
    "def parse_html_and_save_to_csv(html_file_path, csv_filename=\"nba_scores.csv\"):\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "    team1 = \"Atlanta Hawks\"\n",
    "    csv_data = [[\"Date\", \"Team1\", \"Q1_Team1\", \"Q2_Team1\", \"Q3_Team1\", \"Q4_Team1\"]]\n",
    "\n",
    "    game_rows = soup.find_all('tr', id=lambda x: x and x.startswith('game_'))\n",
    "\n",
    "    for tr in game_rows:\n",
    "        date = tr.find('td', class_='text-center').get_text(strip=True)\n",
    "\n",
    "        # Initialize quarter scores with placeholders\n",
    "        q_scores = [\"\"] * 4\n",
    "\n",
    "        # Locate the corresponding detailed score row by ID reference\n",
    "        detailed_score_id = \"score_\" + tr['id'].split('_')[2]\n",
    "        detailed_score_row = soup.find('tr', id=detailed_score_id)\n",
    "        \n",
    "        if detailed_score_row:\n",
    "            detailed_scores = detailed_score_row.find('table')\n",
    "            if detailed_scores:\n",
    "                score_rows = detailed_scores.find_all('tr')[1:3]  # Skip header row\n",
    "                \n",
    "                for index, row in enumerate(score_rows):\n",
    "                    tds = row.find_all('td')[1:5]  # Focus on quarter scores\n",
    "                    for i, td in enumerate(tds):\n",
    "                        if index == 0:  # Assuming the first row is for team1\n",
    "                            q_scores[i] = td.get_text(strip=True)\n",
    "\n",
    "        csv_data.append([date, team1] + q_scores)\n",
    "\n",
    "    with open(csv_filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(csv_data)\n",
    "    print(f\"Data saved to {csv_filename}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = 'https://champsorchumps.us/team/nba/atlanta-hawks/2023'\n",
    "    html_file_path = download_html(url)\n",
    "\n",
    "    if html_file_path:\n",
    "        parse_html_and_save_to_csv(html_file_path)\n",
    "\n",
    "        os.remove(html_file_path)\n",
    "        print(f\"Temporary HTML file deleted: {html_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa730419-0f50-4efc-a05b-5149cd212c42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a05173b-2699-483e-b35c-6de7557a694d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b2a8dda-aaa4-490e-ab1f-081328e698d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table:\n",
      "Q1_Team1, Q1_Team2, Q2_Team1, Q2_Team2, Q3_Team1, Q3_Team2, Q4_Team1, Q4_Team2\n",
      "20, 26, 30, 33, 30, 25, 27, 33\n",
      "\n",
      "---\n",
      "\n",
      "Table:\n",
      "Q1_Team1, Q1_Team2, Q2_Team1, Q2_Team2, Q3_Team1, Q3_Team2, Q4_Team1, Q4_Team2\n",
      "27, 19, 27, 31, 27, 30, 17, 28\n",
      "\n",
      "---\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to save the tables to a CSV file? Enter 'yes' or 'no':  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables saved to nba_scores.csv.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "# URL of the page you want to scrape\n",
    "url = 'https://champsorchumps.us/team/nba/atlanta-hawks/2023'\n",
    "\n",
    "# Headers to mimic a real user visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "# Use requests to retrieve data from a given URL\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Parse the content of the request with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all 'table' tags with a specific class\n",
    "tables = soup.find_all('table', class_='table table-sm pb-0 mb-0 text-center lines-table')\n",
    "\n",
    "# Initialize a list to hold processed data for CSV\n",
    "csv_data = [[\"Q1_Team1\", \"Q1_Team2\", \"Q2_Team1\", \"Q2_Team2\", \"Q3_Team1\", \"Q3_Team2\", \"Q4_Team1\", \"Q4_Team2\"]]\n",
    "\n",
    "# Process each table\n",
    "for table in tables:\n",
    "    rows = table.find_all('tr')[1:3]  # Focus on the two team rows\n",
    "    if len(rows) != 2:\n",
    "        continue  # Skip tables that don't have exactly two team rows\n",
    "    scores = [[td.text.strip() for td in row.find_all('td')[1:5]] for row in rows]  # Extract scores for quarters 1-4\n",
    "    scores_flattened = [item for sublist in zip(*scores) for item in sublist]  # Flatten and alternate scores\n",
    "    csv_data.append(scores_flattened)\n",
    "\n",
    "    # For demonstration, print the first two tables in the console\n",
    "    if len(csv_data) <= 3:  # Includes header row, so limit to 3 for two tables\n",
    "        print(\"Table:\")\n",
    "        print(\", \".join(csv_data[0]))  # Print header\n",
    "        print(\", \".join(scores_flattened))\n",
    "        print(\"\\n---\\n\")\n",
    "\n",
    "# Function to save data to CSV\n",
    "def save_to_csv(data, filename):\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(data)\n",
    "\n",
    "# Ask user if they want to save the tables to a CSV file\n",
    "user_input = input(\"Do you want to save the tables to a CSV file? Enter 'yes' or 'no': \").strip().lower()\n",
    "if user_input == 'yes':\n",
    "    save_to_csv(csv_data, \"nba_scores.csv\")\n",
    "    print(\"Tables saved to nba_scores.csv.\")\n",
    "else:\n",
    "    print(\"Tables were not saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e96b7bb-1237-4bc9-b409-f47d8224665a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329ca35f-bb9f-4ce0-8ea9-b003b00b79c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3fe1db94-347b-4d7d-a178-693ee5c82061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a team from the NBA list:  atlanta-hawks\n",
      "Enter start year:  2024\n",
      "Enter end year:  2024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of data to be saved:\n",
      "Year, Q1_Team1, Q1_Team2, Q2_Team1, Q2_Team2, Q3_Team1, Q3_Team2, Q4_Team1, Q4_Team2\n",
      "2024, 29, 25, 23, 26, 21, 27, 37, 38\n",
      "2024, 35, 31, 34, 30, 31, 31, 26, 28\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to save the tables to a CSV file? Enter 'yes' or 'no':  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables saved to atlanta-hawks_nba_scores_2024_to_2024.csv.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# List of NBA teams for user to choose from\n",
    "nba_teams = [\n",
    "    'atlanta-hawks', 'boston-celtics', 'brooklyn-nets', 'charlotte-hornets',\n",
    "    'chicago-bulls', 'cleveland-cavaliers', 'dallas-mavericks', 'denver-nuggets',\n",
    "    'detroit-pistons', 'golden-state-warriors', 'houston-rockets', 'indiana-pacers',\n",
    "    'los-angeles-clippers', 'los-angeles-lakers', 'memphis-grizzlies', 'miami-heat',\n",
    "    'milwaukee-bucks', 'minnesota-timberwolves', 'new-orleans-pelicans', 'new-york-knicks',\n",
    "    'oklahoma-city-thunder', 'orlando-magic', 'philadelphia-76ers', 'phoenix-suns',\n",
    "    'portland-trail-blazers', 'sacramento-kings', 'san-antonio-spurs', 'toronto-raptors',\n",
    "    'utah-jazz', 'washington-wizards'\n",
    "]\n",
    "\n",
    "# Ask user for team, start year, and end year\n",
    "team = input(\"Enter a team from the NBA list: \").strip().lower().replace(\" \", \"-\")\n",
    "if team not in nba_teams:\n",
    "    print(\"Team not found in the list. Please enter a valid NBA team.\")\n",
    "    exit()\n",
    "start_year = int(input(\"Enter start year: \"))\n",
    "end_year = int(input(\"Enter end year: \"))\n",
    "\n",
    "# Headers to mimic a real user visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "# Initialize a list to hold processed data for CSV\n",
    "csv_data = [[\"Year\", \"Q1_Team1\", \"Q1_Team2\", \"Q2_Team1\", \"Q2_Team2\", \"Q3_Team1\", \"Q3_Team2\", \"Q4_Team1\", \"Q4_Team2\"]]\n",
    "\n",
    "# Loop through each year and scrape data\n",
    "for year in range(start_year, end_year + 1):\n",
    "    url = f\"https://champsorchumps.us/team/nba/{team}/{year}\"\n",
    "    \n",
    "    # Use requests to retrieve data from a given URL\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Parse the content of the request with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all 'table' tags with a specific class\n",
    "    tables = soup.find_all('table', class_='table table-sm pb-0 mb-0 text-center lines-table')\n",
    "    \n",
    "    # Process each table\n",
    "    for table in tables:\n",
    "        rows = table.find_all('tr')[1:3]  # Focus on the two team rows\n",
    "        if len(rows) != 2:\n",
    "            continue  # Skip tables that don't have exactly two team rows\n",
    "        scores = [[td.text.strip() for td in row.find_all('td')[1:5]] for row in rows]  # Extract scores for quarters 1-4\n",
    "        scores_flattened = [year] + [item for sublist in zip(*scores) for item in sublist]  # Flatten and alternate scores, prepend year\n",
    "        csv_data.append(scores_flattened)\n",
    "\n",
    "# Function to save data to CSV\n",
    "def save_to_csv(data, filename):\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(data)\n",
    "\n",
    "# Show table head (first few rows for preview)\n",
    "print(\"Preview of data to be saved:\")\n",
    "for row in csv_data[:3]:  # Show header and first two rows of actual data\n",
    "    print(\", \".join(map(str, row)))\n",
    "\n",
    "# Ask user if they want to save the tables to a CSV file\n",
    "user_input = input(\"Do you want to save the tables to a CSV file? Enter 'yes' or 'no': \").strip().lower()\n",
    "if user_input == 'yes':\n",
    "    filename = f\"{team}_nba_scores_{start_year}_to_{end_year}.csv\"\n",
    "    save_to_csv(csv_data, filename)\n",
    "    print(f\"Tables saved to {filename}.\")\n",
    "else:\n",
    "    print(\"Tables were not saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43c94ce-24b4-47e7-acc7-2d2fe3ffb6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d365e9-bf4f-4213-ba63-4c699be0cf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 feb work, get Q scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01e7c63d-3851-4b36-8959-cd6e79c17f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to nba_scores.csv.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "import tempfile\n",
    "import requests\n",
    "\n",
    "def download_and_parse_html(url, csv_filename=\"nba_scores.csv\"):\n",
    "    # Download HTML content\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve the page.\")\n",
    "        return\n",
    "\n",
    "    # Use BeautifulSoup to parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    team1 = \"Atlanta Hawks\"\n",
    "    csv_data = [[\"Date\", \"Team1\", \"Q1_Team1\", \"Q2_Team1\", \"Q3_Team1\", \"Q4_Team1\"]]\n",
    "\n",
    "    # Find all game rows and their corresponding detailed score rows\n",
    "    game_rows = soup.find_all('tr', id=lambda x: x and x.startswith('game_'))\n",
    "\n",
    "    for tr in game_rows:\n",
    "        date = tr.find('td', class_='text-center').get_text(strip=True)\n",
    "\n",
    "        # Locate the corresponding detailed score row by ID reference\n",
    "        detailed_score_id = \"score_\" + tr['id'].split('_')[2]\n",
    "        detailed_score_row = soup.find('tr', id=detailed_score_id, style=\"display: none;\")\n",
    "\n",
    "        q_scores = [\"\"] * 4\n",
    "        if detailed_score_row:\n",
    "            score_table = detailed_score_row.find('table')\n",
    "            if score_table:\n",
    "                hawks_row = score_table.find_all('tr', class_='font-weight-bold')\n",
    "                if hawks_row:\n",
    "                    q_scores = [td.get_text(strip=True) for td in hawks_row[0].find_all('td')[1:5]]\n",
    "\n",
    "        csv_data.append([date, team1] + q_scores)\n",
    "\n",
    "    # Save the extracted data to a CSV file\n",
    "    with open(csv_filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(csv_data)\n",
    "    print(f\"Data saved to {csv_filename}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = 'https://champsorchumps.us/team/nba/atlanta-hawks/2023'\n",
    "    download_and_parse_html(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97de063-35f7-43e8-b248-5490ead8bc6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a8d7bf-872b-4bf0-ac68-407a142b1164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f7986e9-7abc-4d43-b4dd-7ca7c9ec5517",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path_to_your_first_csv_file.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m csv2_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath_to_your_second_csv_file.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load CSVs into DataFrames\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m df_csv1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv1_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m df_csv2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv2_file_path)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Merge the DataFrames by adding the columns from CSV 2 to CSV 1\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_your_first_csv_file.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have file paths for CSV 1 and CSV 2\n",
    "csv1_file_path = 'path_to_your_first_csv_file.csv'\n",
    "csv2_file_path = 'path_to_your_second_csv_file.csv'\n",
    "\n",
    "# Load CSVs into DataFrames\n",
    "df_csv1 = pd.read_csv(csv1_file_path)\n",
    "df_csv2 = pd.read_csv(csv2_file_path)\n",
    "\n",
    "# Merge the DataFrames by adding the columns from CSV 2 to CSV 1\n",
    "df_merged = pd.concat([df_csv1, df_csv2], axis=1)\n",
    "\n",
    "# If there are any missing values in the merged DataFrame, fill them with 0\n",
    "df_merged.fillna(0, inplace=True)\n",
    "\n",
    "# Now, save the merged DataFrame to a new CSV file without altering the original data\n",
    "output_file_path = 'merged_csv_output.csv'\n",
    "df_merged.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Merged CSV saved as '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992071c4-22bd-4031-be90-45a62b87f8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c27e88-da32-48f1-9f70-7237c7b60694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77462df8-4a74-4aa6-aa7c-eeefd3786a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5fef22-6912-45b1-b5ca-939a7cc5879f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fa980e-b804-4042-b02a-526a82392789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e8a044-68d5-43f5-b03e-ae6028c663df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809c3c4e-e34f-4db8-96b3-372e138260e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818cd5dd-02a3-401d-a417-01194ab36be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "84dc9ea3-1bd0-45df-ae53-ac1aee5bf532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter 'all' for all teams or specify a team (e.g., 'los-angeles-clippers'):  ALL\n",
      "Enter your start year:  2023\n",
      "Enter your end year:  2024\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 104\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m team_selection \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m team \u001b[38;5;129;01min\u001b[39;00m nba_teams:\n\u001b[1;32m--> 104\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_data_for_team\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_year\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_year\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m         master_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([master_df, df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[89], line 59\u001b[0m, in \u001b[0;36mscrape_data_for_team\u001b[1;34m(team, start_year, end_year)\u001b[0m\n\u001b[0;32m     57\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://champsorchumps.us/team/nba/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mteam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     58\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMozilla/5.0\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m---> 59\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m     62\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    787\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    791\u001b[0m     conn,\n\u001b[0;32m    792\u001b[0m     method,\n\u001b[0;32m    793\u001b[0m     url,\n\u001b[0;32m    794\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    795\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    796\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    797\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    798\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    799\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    800\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    801\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    802\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    803\u001b[0m )\n\u001b[0;32m    805\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    806\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    460\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1373\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1375\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define the NBA teams\n",
    "nba_teams = [\n",
    "    'atlanta-hawks', 'boston-celtics', 'brooklyn-nets', 'charlotte-hornets',\n",
    "    'chicago-bulls', 'cleveland-cavaliers', 'dallas-mavericks', 'denver-nuggets',\n",
    "    'detroit-pistons', 'golden-state-warriors', 'houston-rockets', 'indiana-pacers',\n",
    "    'los-angeles-clippers', 'los-angeles-lakers', 'memphis-grizzlies', 'miami-heat',\n",
    "    'milwaukee-bucks', 'minnesota-timberwolves', 'new-orleans-pelicans', 'new-york-knicks',\n",
    "    'oklahoma-city-thunder', 'orlando-magic', 'philadelphia-76ers', 'phoenix-suns',\n",
    "    'portland-trail-blazers', 'sacramento-kings', 'san-antonio-spurs', 'toronto-raptors',\n",
    "    'utah-jazz', 'washington-wizards'\n",
    "]\n",
    "\n",
    "def process_game_row(tr, team):\n",
    "    date = tr.find('td', class_='text-center').get_text(strip=True)\n",
    "    opponent_td = tr.find_all('td', class_='text-center')[1]\n",
    "    opponent_raw = opponent_td.get_text(strip=True)\n",
    "    score = tr.find_all('td', class_='text-center')[2].get_text(strip=True)\n",
    "    is_playoff = opponent_td.find('span', class_='fa fa-bolt') is not None\n",
    "    type = \"1\" if is_playoff else \"0\"\n",
    "    win_loss = \"Win\" if 'W' in score else \"Loss\"\n",
    "    team1 = team.replace('-', ' ')\n",
    "    opponent = opponent_raw.replace('@ ', '').replace('vs ', '').replace('-', ' ')\n",
    "    \n",
    "    ot_game = 0  # Default to no OT\n",
    "    expanding_td = tr.find('td', class_='expanding')\n",
    "    if expanding_td and 'OT' in expanding_td.text:\n",
    "        ot_game = 1  # Mark as OT game\n",
    "\n",
    "    return {\n",
    "        'Date': date, 'Score': score, 'Win': win_loss, 'Type': type,\n",
    "        'Team1': team1, 'Team2': opponent, 'OT': ot_game\n",
    "    }\n",
    "\n",
    "def scrape_data_for_year(team, year):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    url = f\"https://champsorchumps.us/team/nba/{team}/{year}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        target_div = soup.find('div', class_='col-md-12 col-lg-8')\n",
    "        if target_div:\n",
    "            game_rows = target_div.find_all('tr', id=lambda x: x and x.startswith('game_'))\n",
    "            print(f\"Scraping data for {team}. Year {year}.\")\n",
    "            return [process_game_row(tr, team) for tr in game_rows if tr.find('td', class_='text-center')]\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the web page for year {year}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def scrape_data_for_team(team, start_year, end_year):\n",
    "    all_games_data = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        url = f\"https://champsorchumps.us/team/nba/{team}/{year}\"\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            game_rows = soup.find_all('tr', id=lambda x: x and x.startswith('game_'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            for tr in game_rows:\n",
    "                # Extract actual game details\n",
    "                date = tr.find('td', class_='text-center').get_text(strip=True)  # Adjust class as per actual HTML\n",
    "                opponent = tr.find('td', class_='text-center').get_text(strip=True)  # Adjust class\n",
    "                score = tr.find('td', class_='text-center').get_text(strip=True)  # Adjust class\n",
    "                is_playoff = \"Playoff\" if tr.find('span', class_='fa fa-bolt') else \"Regular\"\n",
    "                win_loss = \"Win\" if 'W' in score else \"Loss\"\n",
    "                ot_game = 1 if \"OT\" in score else 0\n",
    "                \n",
    "                # Extract quarter scores\n",
    "                quarter_scores = tr.find_all('td', class_='quarter-score-class')  # Adjust class\n",
    "                q_scores = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  # Default quarter scores\n",
    "                for i, q_score in enumerate(quarter_scores[:8]):\n",
    "                    q_scores[i] = float(q_score.get_text(strip=True))\n",
    "                \n",
    "                game_data = [date, score, win_loss, is_playoff, team.replace('-', ' ').title(), opponent, ot_game] + q_scores\n",
    "                all_games_data.append(game_data)\n",
    "                \n",
    "            time.sleep(1)  # Respectful delay\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for {team} in {year}. Status code: {response.status_code}\")\n",
    "    \n",
    "    columns = [\"Date\", \"Score\", \"Win\", \"Type\", \"Team1\", \"Team2\", \"OT\", \"Q1_Team1\", \"Q1_Team2\", \"Q2_Team1\", \"Q2_Team2\", \"Q3_Team1\", \"Q3_Team2\", \"Q4_Team1\", \"Q4_Team2\"]\n",
    "    return pd.DataFrame(all_games_data, columns=columns)\n",
    "\n",
    "\n",
    "team_selection = input(\"Enter 'all' for all teams or specify a team (e.g., 'los-angeles-clippers'): \").strip().lower().replace(' ', '-')\n",
    "start_year = int(input(\"Enter your start year: \"))\n",
    "end_year = int(input(\"Enter your end year: \"))\n",
    "\n",
    "master_df = pd.DataFrame()\n",
    "\n",
    "if team_selection == 'all':\n",
    "    for team in nba_teams:\n",
    "        df = scrape_data_for_team(team, start_year, end_year)\n",
    "        master_df = pd.concat([master_df, df], ignore_index=True)\n",
    "else:\n",
    "    if team_selection in nba_teams:\n",
    "        master_df = scrape_data_for_team(team_selection, start_year, end_year)\n",
    "    else:\n",
    "        print(\"Team not found. Please ensure you've entered the team name correctly.\")\n",
    "\n",
    "# Remove rows with an empty 'Score' column\n",
    "master_df = master_df[master_df['Score'].str.strip() != '']\n",
    "\n",
    "# Data post-processing\n",
    "master_df['Win'] = master_df['Win'].apply(lambda x: 1 if x == 'Win' else 0)\n",
    "master_df['Type'] = master_df['Type'].apply(lambda x: 1 if x == 'Playoff' else 0)\n",
    "\n",
    "# Export to CSV\n",
    "csv_file_name = f\"nba_data_{start_year}_to_{end_year}.csv\"\n",
    "master_df.to_csv(csv_file_name, index=False)\n",
    "print(f\"CSV file generated: {csv_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "458deba2-b85a-412e-8388-be2359225160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de586616-b4f7-4078-bf08-c79dbc059d21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7d40884-14f6-4d01-8ace-f7e18789838c",
   "metadata": {},
   "source": [
    "# Try this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec7d5240-0c87-4bae-a615-1c9b8c08fbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a team from the NBA list:  atlanta-hawks\n",
      "Enter start year:  2023\n",
      "Enter end year:  2024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of data to be saved:\n",
      "Year, Team1, Q1_Team1, Q2_Team1, Q3_Team1, Q4_Team1, Team2, Q1_Team2, Q2_Team2, Q3_Team2, Q4_Team2\n",
      "2023, Rockets, 20, 30, 30, 27, Hawks, 26, 33, 25, 33\n",
      "2023, Magic, 27, 27, 27, 17, Hawks, 19, 31, 30, 28\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to save the tables to a CSV file? Enter 'yes' or 'no':  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables saved to atlanta-hawks_nba_scores_2023_to_2024.csv.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# List of NBA teams for user to choose from\n",
    "nba_teams = [\n",
    "    'atlanta-hawks', 'boston-celtics', 'brooklyn-nets', 'charlotte-hornets',\n",
    "    'chicago-bulls', 'cleveland-cavaliers', 'dallas-mavericks', 'denver-nuggets',\n",
    "    'detroit-pistons', 'golden-state-warriors', 'houston-rockets', 'indiana-pacers',\n",
    "    'los-angeles-clippers', 'los-angeles-lakers', 'memphis-grizzlies', 'miami-heat',\n",
    "    'milwaukee-bucks', 'minnesota-timberwolves', 'new-orleans-pelicans', 'new-york-knicks',\n",
    "    'oklahoma-city-thunder', 'orlando-magic', 'philadelphia-76ers', 'phoenix-suns',\n",
    "    'portland-trail-blazers', 'sacramento-kings', 'san-antonio-spurs', 'toronto-raptors',\n",
    "    'utah-jazz', 'washington-wizards'\n",
    "]\n",
    "\n",
    "# Ask user for team, start year, and end year\n",
    "team = input(\"Enter a team from the NBA list: \").strip().lower().replace(\" \", \"-\")\n",
    "if team not in nba_teams:\n",
    "    print(\"Team not found in the list. Please enter a valid NBA team.\")\n",
    "    exit()\n",
    "start_year = int(input(\"Enter start year: \"))\n",
    "end_year = int(input(\"Enter end year: \"))\n",
    "\n",
    "# Headers to mimic a real user visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "# Initialize a list to hold processed data for CSV\n",
    "csv_data = [[\"Year\", \"Team1\", \"Q1_Team1\", \"Q2_Team1\", \"Q3_Team1\", \"Q4_Team1\", \"Team2\", \"Q1_Team2\", \"Q2_Team2\", \"Q3_Team2\", \"Q4_Team2\"]]\n",
    "\n",
    "# Loop through each year and scrape data\n",
    "for year in range(start_year, end_year + 1):\n",
    "    url = f\"https://champsorchumps.us/team/nba/{team}/{year}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tables = soup.find_all('table', class_='table table-sm pb-0 mb-0 text-center lines-table')\n",
    "    \n",
    "    for table in tables:\n",
    "        rows = table.find_all('tr')[1:3]  # Focus on the two team rows\n",
    "        if len(rows) != 2:\n",
    "            continue  # Skip tables that don't have exactly two team rows\n",
    "        \n",
    "        # Extract team names and scores\n",
    "        team_names = [row.find('td', class_='text-left').text.strip() for row in rows]\n",
    "        scores = [[td.text.strip() for td in row.find_all('td')[1:5]] for row in rows]\n",
    "        \n",
    "        # Determine unique team names and assign to Team1 and Team2\n",
    "        unique_team_names = list(set(team_names))\n",
    "        if len(unique_team_names) != 2:\n",
    "            continue  # Skip if we don't have exactly two unique team names\n",
    "        \n",
    "        # Flatten and alternate scores, prepend year and team names\n",
    "        scores_flattened = [year] + [unique_team_names[0]] + scores[0] + [unique_team_names[1]] + scores[1]\n",
    "        csv_data.append(scores_flattened)\n",
    "\n",
    "# Function to save data to CSV\n",
    "def save_to_csv(data, filename):\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(data)\n",
    "\n",
    "# Show table head (first few rows for preview)\n",
    "print(\"Preview of data to be saved:\")\n",
    "for row in csv_data[:3]:  # Show header and first two rows of actual data\n",
    "    print(\", \".join(map(str, row)))\n",
    "\n",
    "# Ask user if they want to save the tables to a CSV file\n",
    "user_input = input(\"Do you want to save the tables to a CSV file? Enter 'yes' or 'no': \").strip().lower()\n",
    "if user_input == 'yes':\n",
    "    filename = f\"{team}_nba_scores_{start_year}_to_{end_year}.csv\"\n",
    "    save_to_csv(csv_data, filename)\n",
    "    print(f\"Tables saved to {filename}.\")\n",
    "else:\n",
    "    print(\"Tables were not saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3abc63da-7e49-447d-ae10-7e4f6f021d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a team from the NBA list:  atlanta-hawks\n",
      "Enter start year:  2023\n",
      "Enter end year:  2024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping inconsistent game data for year 2023: [2023, 'Rockets', '20', '30', '30', '27', '107']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '26', '33', '25', '33', '117']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Magic', '27', '27', '27', '17', '98']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '19', '31', '30', '28', '108']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hornets', '22', '37', '45', '22', '126']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '30', '25', '29', '25', '109']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '36', '26', '27', '29', '118']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Pistons', '32', '29', '28', '24', '113']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '35', '34', '37', '30', '136']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Pistons', '29', '38', '27', '18', '112']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '30', '21', '33', '31', '115']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Bucks', '32', '27', '32', '32', '123']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '28', '25', '27', '29', '109']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Raptors', '34', '30', '31', '44', '139']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '21', '36', '32', '23', '112']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Knicks', '32', '33', '10', '24', '99']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Pelicans', '28', '24', '33', '24', '12']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '29', '31', '28', '21', '15']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Bucks', '36', '22', '22', '18', '98']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '25', '27', '37', '28', '117']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Jazz', '26', '37', '22', '40', '125']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '22', '32', '36', '29', '119']\n",
      "Skipping inconsistent game data for year 2023: [2023, '76ers', '20', '22', '24', '29', '95']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '29', '17', '34', '24', '104']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '24', '27', '33', '25', '109']\n",
      "Skipping inconsistent game data for year 2023: [2023, '76ers', '37', '30', '32', '22', '121']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '29', '34', '31', '27', '121']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Bucks', '24', '26', '30', '26', '106']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Celtics', '30', '32', '35', '29', '126']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '18', '35', '25', '23', '101']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Raptors', '31', '31', '27', '22', '11']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '29', '26', '27', '29', '13']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '26', '33', '23', '20', '102']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Cavaliers', '38', '26', '21', '29', '114']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Kings', '26', '31', '31', '18', '106']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '35', '37', '25', '18', '115']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '33', '39', '32', '18', '122']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Rockets', '38', '31', '25', '34', '128']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Heat', '23', '28', '34', '21', '106']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '31', '29', '21', '17', '98']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '34', '21', '26', '20', '101']\n",
      "Skipping inconsistent game data for year 2023: [2023, '76ers', '23', '28', '30', '23', '104']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '38', '32', '30', '25', '125']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Magic', '27', '23', '26', '32', '108']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Nuggets', '25', '21', '35', '28', '109']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '28', '32', '30', '27', '117']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Thunder', '27', '27', '30', '37', '121']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '29', '30', '29', '26', '114']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '20', '21', '26', '22', '89']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Knicks', '31', '22', '36', '24', '113']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '30', '34', '26', '26', '116']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Nets', '36', '32', '28', '24', '120']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Bulls', '27', '23', '28', '32', '12']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '22', '36', '27', '25', '13']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '29', '22', '25', '27', '103']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Grizzlies', '35', '27', '40', '26', '128']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '22', '40', '32', '30', '124']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Magic', '50', '26', '29', '30', '135']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '38', '37', '23', '27', '125']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hornets', '24', '35', '33', '14', '106']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Magic', '27', '38', '29', '31', '125']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '34', '34', '36', '22', '126']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Bulls', '30', '31', '20', '29', '110']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '25', '26', '26', '31', '108']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Pistons', '34', '29', '18', '24', '105']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '32', '28', '35', '35', '130']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '27', '27', '32', '28', '114']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Pacers', '30', '34', '32', '33', '129']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Nets', '31', '25', '27', '25', '108']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '30', '33', '17', '27', '107']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Lakers', '23', '39', '36', '32', '130']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '33', '34', '28', '26', '121']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '26', '27', '42', '26', '11']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Warriors', '38', '32', '25', '26', '11']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '30', '31', '30', '29', '120']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Kings', '30', '28', '30', '29', '117']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '28', '21', '33', '32', '114']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Lakers', '37', '33', '28', '32', '130']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '25', '41', '19', '27', '112']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Clippers', '26', '26', '35', '21', '108']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Bucks', '39', '28', '22', '25', '114']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '19', '27', '36', '23', '105']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '36', '19', '29', '29', '113']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Pacers', '27', '31', '31', '22', '111']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '34', '35', '27', '18', '114']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Raptors', '25', '32', '22', '24', '103']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Heat', '26', '24', '30', '33', '113']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '38', '32', '21', '30', '121']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '40', '25', '33', '32', '130']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Mavericks', '37', '25', '35', '25', '122']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Knicks', '38', '33', '30', '23', '124']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '35', '33', '34', '37', '139']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hornets', '25', '24', '36', '37', '122']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '31', '34', '28', '25', '118']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '21', '27', '30', '22', '100']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Bulls', '17', '40', '21', '33', '111']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '39', '38', '32', '28', '137']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Thunder', '43', '34', '27', '28', '132']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Clippers', '22', '36', '26', '36', '120']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '28', '25', '28', '32', '113']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '23', '37', '32', '33', '125']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Trail Blazers', '31', '33', '35', '30', '129']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '26', '40', '36', '30', '132']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Suns', '23', '24', '20', '33', '100']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '32', '30', '29', '24', '115']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Jazz', '22', '28', '28', '30', '108']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '30', '22', '34', '22', '108']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Nuggets', '35', '35', '27', '31', '128']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '35', '24', '22', '26', '107']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Pelicans', '33', '29', '29', '25', '116']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Suns', '22', '25', '28', '32', '107']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '36', '20', '37', '23', '116']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Spurs', '31', '29', '17', '29', '106']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '32', '29', '32', '32', '125']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '27', '41', '28', '42', '138']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hornets', '36', '38', '35', '35', '144']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Knicks', '37', '29', '25', '31', '122']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '21', '23', '32', '25', '101']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Cavaliers', '23', '34', '26', '36', '119']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '32', '49', '27', '28', '136']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Nets', '29', '28', '37', '33', '127']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '37', '27', '33', '32', '129']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Wizards', '27', '25', '33', '34', '119']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '27', '29', '34', '26', '116']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Trail Blazers', '19', '30', '27', '35', '111']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '31', '28', '38', '32', '129']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '26', '23', '26', '34', '109']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Heat', '29', '37', '21', '30', '117']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '43', '25', '28', '32', '128']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Heat', '30', '33', '29', '38', '130']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '34', '24', '32', '32', '122']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Wizards', '28', '33', '33', '26', '120']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '24', '32', '29', '29', '114']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Wizards', '29', '21', '25', '32', '107']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Celtics', '37', '34', '30', '33', '134']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '38', '23', '28', '36', '125']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Timberwolves', '40', '36', '30', '30', '136']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '29', '23', '33', '30', '115']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Warriors', '31', '35', '30', '23', '119']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '39', '32', '33', '23', '127']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '40', '43', '20', '15', '118']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Spurs', '34', '27', '39', '26', '126']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Pistons', '32', '27', '19', '29', '107']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '38', '28', '33', '30', '129']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '40', '22', '35', '27', '124']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Timberwolves', '39', '28', '27', '31', '125']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Pacers', '35', '39', '24', '32', '130']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '39', '33', '33', '38', '143']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Grizzlies', '34', '27', '35', '27', '123']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '27', '29', '33', '30', '119']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Cavaliers', '24', '27', '33', '34', '118']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '24', '35', '35', '26', '120']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '33', '22', '24', '28', '107']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Nets', '25', '34', '42', '23', '124']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Mavericks', '29', '37', '25', '32', '7']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '37', '33', '24', '29', '9']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '36', '29', '25', '33', '123']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Bulls', '26', '27', '27', '25', '105']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Wizards', '32', '28', '30', '26', '116']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '33', '34', '39', '28', '134']\n",
      "Skipping inconsistent game data for year 2023: [2023, '76ers', '32', '21', '33', '33', '17']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '31', '32', '27', '29', '12']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '31', '33', '26', '24', '114']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Celtics', '29', '32', '39', '20', '120']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '36', '29', '26', '25', '116']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Heat', '27', '23', '28', '27', '105']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '19', '25', '31', '24', '99']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Celtics', '29', '45', '20', '18', '112']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '25', '24', '32', '25', '106']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Celtics', '28', '33', '29', '29', '119']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Celtics', '37', '30', '26', '29', '122']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '33', '41', '26', '30', '130']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Celtics', '35', '30', '27', '37', '129']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '25', '28', '34', '34', '121']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '30', '28', '24', '37', '119']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Celtics', '27', '39', '26', '25', '117']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Celtics', '35', '33', '30', '30', '128']\n",
      "Skipping inconsistent game data for year 2023: [2023, 'Hawks', '34', '33', '33', '20', '120']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '29', '23', '21', '37', '110']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hornets', '25', '26', '27', '38', '116']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Knicks', '35', '34', '31', '26', '126']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '31', '30', '31', '28', '120']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '31', '37', '33', '26', '127']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Bucks', '25', '22', '33', '30', '110']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Timberwolves', '42', '37', '19', '15', '113']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '35', '25', '38', '29', '127']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Wizards', '30', '21', '31', '39', '121']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '25', '34', '40', '31', '130']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '27', '26', '41', '29', '123']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Pelicans', '35', '25', '23', '22', '105']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '28', '22', '27', '40', '117']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Thunder', '29', '29', '38', '30', '126']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '34', '39', '20', '27', '120']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Magic', '34', '35', '29', '21', '119']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Heat', '41', '22', '30', '24', '117']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '24', '28', '30', '27', '109']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '38', '26', '33', '29', '126']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Pistons', '29', '31', '31', '29', '120']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Knicks', '33', '24', '30', '29', '116']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '22', '34', '25', '33', '114']\n",
      "Skipping inconsistent game data for year 2024: [2024, '76ers', '30', '27', '37', '32', '126']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '24', '32', '31', '29', '116']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Pacers', '34', '39', '46', '38', '157']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '40', '46', '28', '38', '152']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Nets', '33', '34', '28', '36', '14']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '45', '32', '23', '31', '16']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '34', '36', '37', '29', '136']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Wizards', '30', '28', '20', '30', '108']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '26', '27', '29', '21', '103']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Celtics', '33', '36', '21', '23', '113']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '33', '28', '29', '15', '105']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Cavaliers', '29', '35', '38', '26', '128']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '26', '36', '39', '36', '137']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Spurs', '35', '31', '39', '30', '135']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '37', '26', '35', '23', '121']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Bucks', '42', '25', '32', '33', '132']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Nets', '31', '22', '28', '33', '114']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '33', '23', '26', '31', '113']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '23', '45', '24', '22', '114']\n",
      "Skipping inconsistent game data for year 2024: [2024, '76ers', '35', '27', '31', '32', '125']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Nuggets', '28', '37', '38', '26', '129']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '36', '26', '24', '36', '122']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '39', '27', '29', '33', '128']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Raptors', '31', '33', '39', '32', '135']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '30', '29', '28', '38', '125']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Raptors', '24', '25', '29', '26', '104']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '28', '24', '29', '38', '119']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Cavaliers', '41', '29', '22', '35', '127']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Pistons', '25', '27', '32', '40', '124']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '38', '23', '35', '34', '130']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '39', '38', '21', '36', '134']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Rockets', '29', '36', '35', '27', '127']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '26', '34', '26', '27', '113']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Heat', '33', '29', '26', '34', '122']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Grizzlies', '25', '25', '40', '35', '125']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '32', '28', '33', '26', '119']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '17', '39', '29', '28', '113']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Bulls', '28', '22', '33', '35', '118']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Kings', '23', '30', '33', '31', '117']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '36', '35', '20', '19', '110']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '28', '42', '30', '30', '130']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Wizards', '30', '28', '29', '39', '126']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Thunder', '25', '34', '44', '35', '138']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '39', '37', '40', '25', '141']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '24', '30', '29', '33', '116']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Pacers', '38', '40', '39', '33', '150']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '28', '32', '18', '27', '5']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Magic', '32', '19', '34', '20', '12']\n",
      "Skipping inconsistent game data for year 2024: [2024, '76ers', '31', '33', '34', '27', '7']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '34', '27', '34', '30', '14']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Pacers', '34', '34', '31', '27', '126']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '30', '32', '19', '27', '108']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Wizards', '30', '28', '41', '28', '127']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '25', '22', '24', '28', '99']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Spurs', '16', '18', '33', '32', '99']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '35', '34', '18', '22', '109']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Magic', '26', '25', '25', '28', '104']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '26', '23', '29', '28', '106']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '32', '25', '24', '28', '109']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Heat', '24', '32', '27', '25', '108']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Cavaliers', '25', '34', '34', '23', '116']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '20', '25', '25', '25', '95']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '24', '16', '29', '38', '107']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Kings', '27', '30', '30', '35', '122']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '31', '37', '24', '20', '112']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Warriors', '38', '31', '32', '33', '134']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Mavericks', '27', '39', '42', '40', '148']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '31', '35', '36', '41', '143']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Raptors', '26', '35', '34', '30', '125']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '34', '23', '40', '29', '126']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Lakers', '29', '27', '32', '34', '122']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '36', '31', '36', '35', '138']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Suns', '30', '33', '32', '25', '120']\n",
      "Skipping inconsistent game data for year 2024: [2024, 'Hawks', '33', '33', '34', '29', '129']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to save the tables to a CSV file? Enter 'yes' or 'no':  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to atlanta-hawks_nba_scores_2023_to_2024.csv.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# List of NBA teams for user to choose from\n",
    "nba_teams = [\n",
    "    'atlanta-hawks', 'boston-celtics', 'brooklyn-nets', 'charlotte-hornets',\n",
    "    'chicago-bulls', 'cleveland-cavaliers', 'dallas-mavericks', 'denver-nuggets',\n",
    "    'detroit-pistons', 'golden-state-warriors', 'houston-rockets', 'indiana-pacers',\n",
    "    'los-angeles-clippers', 'los-angeles-lakers', 'memphis-grizzlies', 'miami-heat',\n",
    "    'milwaukee-bucks', 'minnesota-timberwolves', 'new-orleans-pelicans', 'new-york-knicks',\n",
    "    'oklahoma-city-thunder', 'orlando-magic', 'philadelphia-76ers', 'phoenix-suns',\n",
    "    'portland-trail-blazers', 'sacramento-kings', 'san-antonio-spurs', 'toronto-raptors',\n",
    "    'utah-jazz', 'washington-wizards'\n",
    "]\n",
    "\n",
    "team = input(\"Enter a team from the NBA list: \").strip().lower().replace(\" \", \"-\")\n",
    "if team not in nba_teams:\n",
    "    print(\"Team not found in the list. Please enter a valid NBA team.\")\n",
    "    exit()\n",
    "start_year = int(input(\"Enter start year: \"))\n",
    "end_year = int(input(\"Enter end year: \"))\n",
    "\n",
    "# Headers for the HTTP request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "# Initialize CSV data list\n",
    "csv_data = [[\"Year\", \"Team1\", \"Q1_Team1\", \"Q2_Team1\", \"Q3_Team1\", \"Q4_Team1\", \"Total_Team1\", \"Team2\", \"Q1_Team2\", \"Q2_Team2\", \"Q3_Team2\", \"Q4_Team2\", \"Total_Team2\"]]\n",
    "\n",
    "# Function to process and append game data correctly\n",
    "def process_and_append_game_data(year, game_data):\n",
    "    # Ensure the game data aligns with the expected structure before appending\n",
    "    if len(game_data) == 13:  # Expected number of columns based on the header\n",
    "        csv_data.append(game_data)\n",
    "    else:\n",
    "        print(f\"Skipping inconsistent game data for year {year}: {game_data}\")\n",
    "\n",
    "# Loop through each year to scrape and process data\n",
    "for year in range(start_year, end_year + 1):\n",
    "    url = f\"https://champsorchumps.us/team/nba/{team}/{year}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tables = soup.find_all('table', class_='table table-sm pb-0 mb-0 text-center lines-table')\n",
    "    \n",
    "    for table in tables:\n",
    "        rows = table.find_all('tr')\n",
    "        for row in rows:\n",
    "            # Find the team names and scores within this row\n",
    "            cells = row.find_all('td')\n",
    "            if len(cells) > 5:  # Ensuring this row contains score data\n",
    "                team_name = cells[0].text.strip()\n",
    "                scores = [cell.text.strip() for cell in cells[1:6]]  # Scores for Q1-Q4 and Total\n",
    "                if team_name:  # Check if the team name was found\n",
    "                    # Determine if this is Team1 or Team2 based on presence in csv_data\n",
    "                    if len(csv_data[-1]) < 7:  # If the last entry is still filling Team1 data\n",
    "                        game_data = csv_data[-1] + [team_name] + scores\n",
    "                    else:  # Start a new game entry\n",
    "                        game_data = [year, team_name] + scores\n",
    "                    process_and_append_game_data(year, game_data)\n",
    "\n",
    "# Prompt for saving the data to CSV\n",
    "def save_to_csv(filename):\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(csv_data)\n",
    "    print(f\"Data saved to {filename}.\")\n",
    "\n",
    "user_input = input(\"Do you want to save the tables to a CSV file? Enter 'yes' or 'no': \").strip().lower()\n",
    "if user_input == 'yes':\n",
    "    filename = f\"{team}_nba_scores_{start_year}_to_{end_year}.csv\"\n",
    "    save_to_csv(filename)\n",
    "else:\n",
    "    print(\"Data saving cancelled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ca3b6c-d1f5-488b-9599-98b383eca572",
   "metadata": {},
   "source": [
    "# Adjust format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c51a48a-00ad-45a6-b657-8f58843a0596",
   "metadata": {},
   "source": [
    "# USE THIS TO ADJUST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04a4dfce-7ea2-4af7-a241-95ae340ded21",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .str accessor with string values!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTeam2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTeam2\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(normalize_team_name)\u001b[38;5;241m.\u001b[39mmap(teams_id)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Before mapping 'Team2', remove '@' or 'vs' from the beginning\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTeam2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTeam2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^(?:@|vs)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(normalize_team_name)\u001b[38;5;241m.\u001b[39mmap(teams_id)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Correct date conversion, assuming the date is in a recognizable format\u001b[39;00m\n\u001b[0;32m     37\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# Adjust this line based on the actual format of your 'Date' column, if necessary\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:6204\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   6198\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   6199\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   6200\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   6201\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   6202\u001b[0m ):\n\u001b[0;32m   6203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 6204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\accessor.py:224\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor\n\u001b[1;32m--> 224\u001b[0m accessor_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\strings\\accessor.py:190\u001b[0m, in \u001b[0;36mStringMethods.__init__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstring_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringDtype\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_categorical \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype)\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype, StringDtype)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\strings\\accessor.py:244\u001b[0m, in \u001b[0;36mStringMethods._validate\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    241\u001b[0m inferred_dtype \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39minfer_dtype(values, skipna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inferred_dtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_types:\n\u001b[1;32m--> 244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only use .str accessor with string values!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inferred_dtype\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can only use .str accessor with string values!"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dictionary for mapping team names to their IDs\n",
    "teams_id = {\n",
    "    'atlantahawks': 1610612737, 'bostonceltics': 1610612738, 'clevelandcavaliers': 1610612739,\n",
    "    'neworleanspelicans': 1610612740, 'chicagobulls': 1610612741, 'dallasmavericks': 1610612742,\n",
    "    'denvernuggets': 1610612743, 'goldenstatewarriors': 1610612744, 'houstonrockets': 1610612745,\n",
    "    'losangelesclippers': 1610612746, 'losangeleslakers': 1610612747, 'miamiheat': 1610612748,\n",
    "    'milwaukeebucks': 1610612749, 'minnesotatimberwolves': 1610612750, 'brooklynnets': 1610612751,\n",
    "    'newyorkknicks': 1610612752, 'orlandomagic': 1610612753, 'indianapacers': 1610612754,\n",
    "    'philadelphia76ers': 1610612755, 'phoenixsuns': 1610612756, 'portlandtrailblazers': 1610612757,\n",
    "    'sacramentokings': 1610612758, 'sanantoniospurs': 1610612759, 'oklahomacitythunder': 1610612760,\n",
    "    'torontoraptors': 1610612761, 'utahjazz': 1610612762, 'memphisgrizzlies': 1610612763,\n",
    "    'washingtonwizards': 1610612764, 'detroitpistons': 1610612765, 'charlottehornets': 1610612766,\n",
    "}\n",
    "\n",
    "# Assuming 'nba_data_2015_to_2024.csv' is your CSV file\n",
    "df = pd.read_csv('nba_data_2023_to_2024.csv')\n",
    "\n",
    "\n",
    "# Function to normalize team names\n",
    "def normalize_team_name(name):\n",
    "    # Remove '@' or 'vs' at the beginning of the string\n",
    "    name = name.lstrip('@').lstrip('vs').strip()\n",
    "    # Convert to lower case and remove spaces for matching with dictionary keys\n",
    "    return name.replace(\" \", \"\").lower()\n",
    "\n",
    "# Normalize and map 'Team1' and 'Team2' names\n",
    "#df['Team1'] = df['Team1'].apply(normalize_team_name).map(teams_id)\n",
    "\n",
    "df['Team2'] = df['Team2'].str.replace(' ', '', regex=True).apply(normalize_team_name).map(teams_id)\n",
    "\n",
    "# Before mapping 'Team2', remove '@' or 'vs' from the beginning\n",
    "df['Team2'] = df['Team2'].str.replace('^(?:@|vs)\\s*', '', regex=True).apply(normalize_team_name).map(teams_id)\n",
    "# Correct date conversion, assuming the date is in a recognizable format\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])  # Adjust this line based on the actual format of your 'Date' column, if necessary\n",
    "\n",
    "\n",
    "# Extract score values with varying formats\n",
    "score_pattern = r'(\\d{2,3})\\s*[-–]\\s*(\\d{2,3})'\n",
    "scores = df['Score'].str.extract(score_pattern)\n",
    "\n",
    "\n",
    "\n",
    "# Create new columns 'Team1_Score' and 'Team2_Score' and assign extracted values\n",
    "df['Team1_Score'] = scores.iloc[:, 0].astype(float)\n",
    "df['Team2_Score'] = scores.iloc[:, 1].astype(float)\n",
    "\n",
    "# Remove 'Win' or 'Loss' from the \"Score\" column\n",
    "df['Score'] = df['Score'].str.extract('(\\d{2,3}\\s*[-–]\\s*\\d{2,3})')[0]\n",
    "\n",
    "# Calculate average score for each team in \"Team1\"\n",
    "average_scores = df.groupby('Team1')['Team1_Score'].mean().rename('Avg_Score')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate win ratio for each team in \"Team1\"\n",
    "win_ratios = df.groupby('Team1')['Win'].mean().rename('Avg_Win_Ratio')\n",
    "\n",
    "\n",
    "# Initialize dictionaries to hold win/loss counts\n",
    "regular_games = {}\n",
    "playoff_games = {}\n",
    "\n",
    "# Iterate through each row to count wins/losses\n",
    "for index, row in df.iterrows():\n",
    "    team_id = row['Team1']\n",
    "    game_type = 'playoff' if row['Type'] == 1 else 'regular'\n",
    "    win = row['Win'] == 1\n",
    "    \n",
    "    # Initialize team record in dictionaries if not already present\n",
    "    if team_id not in regular_games:\n",
    "        regular_games[team_id] = {'wins': 0, 'losses': 0}\n",
    "    if team_id not in playoff_games:\n",
    "        playoff_games[team_id] = {'wins': 0, 'losses': 0}\n",
    "    \n",
    "    # Increment win/loss count based on game type\n",
    "    if game_type == 'regular':\n",
    "        if win:\n",
    "            regular_games[team_id]['wins'] += 1\n",
    "        else:\n",
    "            regular_games[team_id]['losses'] += 1\n",
    "    elif game_type == 'playoff':\n",
    "        if win:\n",
    "            playoff_games[team_id]['wins'] += 1\n",
    "        else:\n",
    "            playoff_games[team_id]['losses'] += 1\n",
    "\n",
    "# Calculate win ratios for regular and playoff games\n",
    "regular_win_ratios = {team_id: record['wins'] / (record['wins'] + record['losses']) for team_id, record in regular_games.items() if (record['wins'] + record['losses']) > 0}\n",
    "playoff_win_ratios = {team_id: record['wins'] / (record['wins'] + record['losses']) for team_id, record in playoff_games.items() if (record['wins'] + record['losses']) > 0}\n",
    "\n",
    "# Convert win ratio dictionaries to DataFrames for easy merging\n",
    "regular_win_ratios_df = pd.DataFrame(list(regular_win_ratios.items()), columns=['Team1', 'Regular_Win_Ratio'])\n",
    "playoff_win_ratios_df = pd.DataFrame(list(playoff_win_ratios.items()), columns=['Team1', 'Playoff_Win_Ratio'])\n",
    "\n",
    "# Merge win ratios back to the original DataFrame\n",
    "df = pd.merge(df, regular_win_ratios_df, on='Team1', how='left')\n",
    "df = pd.merge(df, playoff_win_ratios_df, on='Team1', how='left')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Calculate the absolute score difference\n",
    "df['Score_Difference'] = abs(df['Team1_Score'] - df['Team2_Score'])\n",
    "\n",
    "# Step 2: Normalize the score difference to a similarity score (0 to 1)\n",
    "# Assuming the maximum score difference observed is a useful normalization factor\n",
    "max_diff = df['Score_Difference'].max()\n",
    "df['Similarity_Score'] = 1 - (df['Score_Difference'] / max_diff)\n",
    "\n",
    "# Note: This is a simple normalization that assumes the max score difference is a good denominator.\n",
    "# This might need to be adjusted based on your specific criteria for similarity.\n",
    "\n",
    "# Drop the 'Score_Difference' column if it's no longer needed\n",
    "df.drop('Score_Difference', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Remove the \"Score\" column\n",
    "df.drop('Score', axis=1, inplace=True)\n",
    "\n",
    "# Join the average score and win ratio back to the main DataFrame\n",
    "df = df.join(average_scores, on='Team1')\n",
    "df = df.join(win_ratios, on='Team1')\n",
    "\n",
    "# Calculate Point Differential\n",
    "df['Point_Differential'] = df['Team2_Score'] - df['Team1_Score']\n",
    "\n",
    "# Display the head of the DataFrame to show the new columns\n",
    "print(df.head())\n",
    "\n",
    "# Conditional save to CSV based on user input\n",
    "save_file = input(\"Do you want to save the updated data to a CSV file? (yes/no): \").strip().lower()\n",
    "if save_file == 'yes':\n",
    "    csv_file_name = input(\"Enter the name for the CSV file (e.g., 'updated_nba_data.csv'): \").strip()\n",
    "    df.to_csv(csv_file_name, index=False)\n",
    "    print(f\"Updated data saved to '{csv_file_name}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbda2a46-ff91-498d-bd74-a5f734a328b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d363ff26-c298-4e5c-a146-5e42f1803856",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "columns overlap but no suffix specified: Index(['Avg_Score_Team1'], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 145\u001b[0m\n\u001b[0;32m    142\u001b[0m avg_scores \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTeam1\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTeam1_Score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mrename(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAvg_Score_Team1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# Join the Avg_Score back to the main DataFrame for Team1\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mavg_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTeam1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# Calculate the average score for each team when it appears as Team1\u001b[39;00m\n\u001b[0;32m    148\u001b[0m avg_scores_team1 \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTeam1\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTeam1_Score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:10412\u001b[0m, in \u001b[0;36mDataFrame.join\u001b[1;34m(self, other, on, how, lsuffix, rsuffix, sort, validate)\u001b[0m\n\u001b[0;32m  10402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m  10403\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m merge(\n\u001b[0;32m  10404\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10405\u001b[0m             other,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10410\u001b[0m             validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m  10411\u001b[0m         )\n\u001b[1;32m> 10412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10413\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m  10418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m  10419\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlsuffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrsuffix\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10420\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10422\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  10423\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m  10424\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:183\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[0;32m    170\u001b[0m         left_df,\n\u001b[0;32m    171\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    181\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m    182\u001b[0m     )\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:885\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indicator_pre_merge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\n\u001b[0;32m    883\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_join_info()\n\u001b[1;32m--> 885\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_and_concat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    888\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:837\u001b[0m, in \u001b[0;36m_MergeOperation._reindex_and_concat\u001b[1;34m(self, join_index, left_indexer, right_indexer, copy)\u001b[0m\n\u001b[0;32m    834\u001b[0m left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft[:]\n\u001b[0;32m    835\u001b[0m right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright[:]\n\u001b[1;32m--> 837\u001b[0m llabels, rlabels \u001b[38;5;241m=\u001b[39m \u001b[43m_items_overlap_with_suffix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuffixes\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m left_indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_range_indexer(left_indexer, \u001b[38;5;28mlen\u001b[39m(left)):\n\u001b[0;32m    842\u001b[0m     \u001b[38;5;66;03m# Pinning the index here (and in the right code just below) is not\u001b[39;00m\n\u001b[0;32m    843\u001b[0m     \u001b[38;5;66;03m#  necessary, but makes the `.take` more performant if we have e.g.\u001b[39;00m\n\u001b[0;32m    844\u001b[0m     \u001b[38;5;66;03m#  a MultiIndex for left.index.\u001b[39;00m\n\u001b[0;32m    845\u001b[0m     lmgr \u001b[38;5;241m=\u001b[39m left\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[0;32m    846\u001b[0m         join_index,\n\u001b[0;32m    847\u001b[0m         left_indexer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    852\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    853\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:2661\u001b[0m, in \u001b[0;36m_items_overlap_with_suffix\u001b[1;34m(left, right, suffixes)\u001b[0m\n\u001b[0;32m   2658\u001b[0m lsuffix, rsuffix \u001b[38;5;241m=\u001b[39m suffixes\n\u001b[0;32m   2660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lsuffix \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rsuffix:\n\u001b[1;32m-> 2661\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns overlap but no suffix specified: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mto_rename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2663\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrenamer\u001b[39m(x, suffix: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   2664\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2665\u001b[0m \u001b[38;5;124;03m    Rename the left and right indices.\u001b[39;00m\n\u001b[0;32m   2666\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[38;5;124;03m    x : renamed column name\u001b[39;00m\n\u001b[0;32m   2678\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: columns overlap but no suffix specified: Index(['Avg_Score_Team1'], dtype='object')"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'nba_data_2015_to_2024.csv' is your CSV file\n",
    "df = pd.read_csv('nba_data_2023_to_2024.csv')\n",
    "\n",
    "# Dictionary for mapping team names to their IDs\n",
    "teams_id = {\n",
    "    'atlantahawks': 1610612737, 'bostonceltics': 1610612738, 'clevelandcavaliers': 1610612739,\n",
    "    'neworleanspelicans': 1610612740, 'chicagobulls': 1610612741, 'dallasmavericks': 1610612742,\n",
    "    'denvernuggets': 1610612743, 'goldenstatewarriors': 1610612744, 'houstonrockets': 1610612745,\n",
    "    'losangelesclippers': 1610612746, 'losangeleslakers': 1610612747, 'miamiheat': 1610612748,\n",
    "    'milwaukeebucks': 1610612749, 'minnesotatimberwolves': 1610612750, 'brooklynnets': 1610612751,\n",
    "    'newyorkknicks': 1610612752, 'orlandomagic': 1610612753, 'indianapacers': 1610612754,\n",
    "    'philadelphia76ers': 1610612755, 'phoenixsuns': 1610612756, 'portlandtrailblazers': 1610612757,\n",
    "    'sacramentokings': 1610612758, 'sanantoniospurs': 1610612759, 'oklahomacitythunder': 1610612760,\n",
    "    'torontoraptors': 1610612761, 'utahjazz': 1610612762, 'memphisgrizzlies': 1610612763,\n",
    "    'washingtonwizards': 1610612764, 'detroitpistons': 1610612765, 'charlottehornets': 1610612766,\n",
    "}\n",
    "\n",
    "# Normalize team names and map them\n",
    "def normalize_team_name(name):\n",
    "    name = name.lstrip('@').lstrip('vs').strip().lower()  # Keep spaces, convert to lowercase\n",
    "    return name.replace(\" \", \"\")\n",
    "\n",
    "df['Team1'] = df['Team1'].apply(normalize_team_name).map(teams_id)\n",
    "df['Team2'] = df['Team2'].str.replace('^(?:@|vs)\\s*', '', regex=True).apply(normalize_team_name).map(teams_id)\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Score extraction\n",
    "score_pattern = r'(\\d+)\\s*[-–]\\s*(\\d+)'\n",
    "scores = df['Score'].str.extract(score_pattern)\n",
    "df['Team1_Score'] = scores[0].astype(float)\n",
    "df['Team2_Score'] = scores[1].astype(float)\n",
    "\n",
    "# Calculate Win/Loss ratio and average scores without causing large memory allocations\n",
    "# Here, assume 'Win' column is a binary indicator (1 for win, 0 for loss)\n",
    "df['Win'] = df['Score'].str.startswith('Win').astype(int)  # Assuming 'Win' prefix indicates a win\n",
    "\n",
    "# Calculate averages and win ratios directly without excessive DataFrame merges\n",
    "df['Team1_ID'] = df['Team1']  # Use team ID directly for grouping\n",
    "average_scores = df.groupby('Team1_ID')['Team1_Score'].mean().rename('Avg_Score')\n",
    "win_ratios = df.groupby('Team1_ID')['Win'].mean().rename('Win_Ratio')\n",
    "\n",
    "# Merge calculated statistics back to the main DataFrame if necessary\n",
    "# For large DataFrames, consider whether this step is essential, as it can be memory-intensive\n",
    "\n",
    "# Simplify the process by directly calculating the required metrics without merging when possible\n",
    "df['Avg_Score_Team1'] = df['Team1_ID'].map(average_scores)\n",
    "df['Win_Ratio_Team1'] = df['Team1_ID'].map(win_ratios)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate win ratio for each team in \"Team1\"\n",
    "win_ratios = df.groupby('Team1')['Win'].mean().rename('Avg_Win_Ratio')\n",
    "\n",
    "\n",
    "# Initialize dictionaries to hold win/loss counts\n",
    "regular_games = {}\n",
    "playoff_games = {}\n",
    "\n",
    "# Iterate through each row to count wins/losses\n",
    "for index, row in df.iterrows():\n",
    "    team_id = row['Team1']\n",
    "    game_type = 'playoff' if row['Type'] == 1 else 'regular'\n",
    "    win = row['Win'] == 1\n",
    "    \n",
    "    # Initialize team record in dictionaries if not already present\n",
    "    if team_id not in regular_games:\n",
    "        regular_games[team_id] = {'wins': 0, 'losses': 0}\n",
    "    if team_id not in playoff_games:\n",
    "        playoff_games[team_id] = {'wins': 0, 'losses': 0}\n",
    "    \n",
    "    # Increment win/loss count based on game type\n",
    "    if game_type == 'regular':\n",
    "        if win:\n",
    "            regular_games[team_id]['wins'] += 1\n",
    "        else:\n",
    "            regular_games[team_id]['losses'] += 1\n",
    "    elif game_type == 'playoff':\n",
    "        if win:\n",
    "            playoff_games[team_id]['wins'] += 1\n",
    "        else:\n",
    "            playoff_games[team_id]['losses'] += 1\n",
    "\n",
    "# Calculate win ratios for regular and playoff games\n",
    "regular_win_ratios = {team_id: record['wins'] / (record['wins'] + record['losses']) for team_id, record in regular_games.items() if (record['wins'] + record['losses']) > 0}\n",
    "playoff_win_ratios = {team_id: record['wins'] / (record['wins'] + record['losses']) for team_id, record in playoff_games.items() if (record['wins'] + record['losses']) > 0}\n",
    "\n",
    "# Convert win ratio dictionaries to DataFrames for easy merging\n",
    "regular_win_ratios_df = pd.DataFrame(list(regular_win_ratios.items()), columns=['Team1', 'Regular_Win_Ratio'])\n",
    "playoff_win_ratios_df = pd.DataFrame(list(playoff_win_ratios.items()), columns=['Team1', 'Playoff_Win_Ratio'])\n",
    "\n",
    "# Merge win ratios back to the original DataFrame\n",
    "df = pd.merge(df, regular_win_ratios_df, on='Team1', how='left')\n",
    "df = pd.merge(df, playoff_win_ratios_df, on='Team1', how='left')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Calculate the absolute score difference\n",
    "df['Score_Difference'] = abs(df['Team1_Score'] - df['Team2_Score'])\n",
    "\n",
    "# Step 2: Normalize the score difference to a similarity score (0 to 1)\n",
    "# Assuming the maximum score difference observed is a useful normalization factor\n",
    "max_diff = df['Score_Difference'].max()\n",
    "df['Similarity_Score'] = 1 - (df['Score_Difference'] / max_diff)\n",
    "\n",
    "# Note: This is a simple normalization that assumes the max score difference is a good denominator.\n",
    "# This might need to be adjusted based on your specific criteria for similarity.\n",
    "\n",
    "# Drop the 'Score_Difference' column if it's no longer needed\n",
    "df.drop('Score_Difference', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Remove the \"Score\" column\n",
    "df.drop('Score', axis=1, inplace=True)\n",
    "\n",
    "# Join the average score and win ratio back to the main DataFrame\n",
    "df = df.join(average_scores, on='Team1')\n",
    "df = df.join(win_ratios, on='Team1')\n",
    "\n",
    "# Here's the new part:\n",
    "# Calculate the Avg_Score for each team when it appears as Team1\n",
    "avg_scores = df.groupby('Team1')['Team1_Score'].mean().rename('Avg_Score_Team1')\n",
    "\n",
    "# Join the Avg_Score back to the main DataFrame for Team1\n",
    "df = df.join(avg_scores, on='Team1')\n",
    "\n",
    "# Calculate the average score for each team when it appears as Team1\n",
    "avg_scores_team1 = df.groupby('Team1')['Team1_Score'].mean()\n",
    "\n",
    "# Convert the Series to a DataFrame for easier manipulation\n",
    "avg_scores_df = avg_scores_team1.reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "avg_scores_df.columns = ['Team1', 'AVG_SCORE']\n",
    "\n",
    "# Create a dictionary from the DataFrame for faster lookup\n",
    "team_avg_score_dict = avg_scores_df.set_index('Team1')['AVG_SCORE'].to_dict()\n",
    "\n",
    "# Function to calculate Avg_Point_Differential\n",
    "def calculate_avg_point_differential(row):\n",
    "    team1_avg_score = team_avg_score_dict.get(row['Team1'], 0)\n",
    "    team2_avg_score = team_avg_score_dict.get(row['Team2'], 0)\n",
    "    return team1_avg_score - team2_avg_score\n",
    "\n",
    "# Calculate Avg_Point_Differential for each row\n",
    "df['Avg_Point_Differential'] = df.apply(calculate_avg_point_differential, axis=1)\n",
    "\n",
    "df['OT'] = ot_column\n",
    "\n",
    "# Display the head of the DataFrame to show the new columns\n",
    "print(df.head())\n",
    "\n",
    "# Ask the user before saving the updated DataFrame to a CSV file\n",
    "save_file = input(\"Do you want to save the updated data to a CSV file? (yes/no): \").strip().lower()\n",
    "if save_file == 'yes':\n",
    "    csv_file_name = input(\"Enter the name for the CSV file (e.g., 'updated_nba_data.csv'): \").strip()\n",
    "    df.to_csv(csv_file_name, index=False)\n",
    "    print(f\"Updated data saved to '{csv_file_name}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc52e107-be67-4016-bcaf-72c1e4b7dd6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "193c6e66-c6c6-4a78-80f8-1eff7039fffc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Column not found: Team1_Score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 89\u001b[0m\n\u001b[0;32m     86\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnba_data_2023_to_2024.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Calculate the Avg_Score for each team when it appears as Team1\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m avg_scores \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTeam1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTeam1_Score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mrename(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAvg_Score_Team1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Join the Avg_Score back to the main DataFrame for Team1\u001b[39;00m\n\u001b[0;32m     92\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mjoin(avg_scores, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTeam1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:1964\u001b[0m, in \u001b[0;36mDataFrameGroupBy.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1958\u001b[0m     \u001b[38;5;66;03m# if len == 1, then it becomes a SeriesGroupBy and this is actually\u001b[39;00m\n\u001b[0;32m   1959\u001b[0m     \u001b[38;5;66;03m# valid syntax, so don't raise\u001b[39;00m\n\u001b[0;32m   1960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1961\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot subset columns with a tuple with more than one element. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1962\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse a list instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1963\u001b[0m     )\n\u001b[1;32m-> 1964\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\base.py:244\u001b[0m, in \u001b[0;36mSelectionMixin.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj:\n\u001b[1;32m--> 244\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    245\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj[key]\u001b[38;5;241m.\u001b[39mndim\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gotitem(key, ndim\u001b[38;5;241m=\u001b[39mndim)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Column not found: Team1_Score'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dictionary for mapping team names to their IDs\n",
    "teams_id = {\n",
    "    'atlantahawks': 1610612737, 'bostonceltics': 1610612738, 'clevelandcavaliers': 1610612739,\n",
    "    'neworleanspelicans': 1610612740, 'chicagobulls': 1610612741, 'dallasmavericks': 1610612742,\n",
    "    'denvernuggets': 1610612743, 'goldenstatewarriors': 1610612744, 'houstonrockets': 1610612745,\n",
    "    'losangelesclippers': 1610612746, 'losangeleslakers': 1610612747, 'miamiheat': 1610612748,\n",
    "    'milwaukeebucks': 1610612749, 'minnesotatimberwolves': 1610612750, 'brooklynnets': 1610612751,\n",
    "    'newyorkknicks': 1610612752, 'orlandomagic': 1610612753, 'indianapacers': 1610612754,\n",
    "    'philadelphia76ers': 1610612755, 'phoenixsuns': 1610612756, 'portlandtrailblazers': 1610612757,\n",
    "    'sacramentokings': 1610612758, 'sanantoniospurs': 1610612759, 'oklahomacitythunder': 1610612760,\n",
    "    'torontoraptors': 1610612761, 'utahjazz': 1610612762, 'memphisgrizzlies': 1610612763,\n",
    "    'washingtonwizards': 1610612764, 'detroitpistons': 1610612765, 'charlottehornets': 1610612766,\n",
    "}\n",
    "\n",
    "# Assuming 'nba_data_2023_to_2024.csv' is your CSV file\n",
    "df = pd.read_csv('nba_data_2023_to_2024.csv')\n",
    "\n",
    "# Calculate the Avg_Score for each team when it appears as Team1\n",
    "avg_scores = df.groupby('Team1')['Team1_Score'].mean().rename('Avg_Score_Team1')\n",
    "\n",
    "# Join the Avg_Score back to the main DataFrame for Team1\n",
    "df = df.join(avg_scores, on='Team1')\n",
    "\n",
    "# Calculate the average score for each team when it appears as Team1\n",
    "avg_scores_team1 = df.groupby('Team1')['Team1_Score'].mean()\n",
    "\n",
    "# Convert the Series to a DataFrame for easier manipulation\n",
    "avg_scores_df = avg_scores_team1.reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "avg_scores_df.columns = ['Team1', 'AVG_SCORE']\n",
    "\n",
    "# Create a dictionary from the DataFrame for faster lookup\n",
    "team_avg_score_dict = avg_scores_df.set_index('Team1')['AVG_SCORE'].to_dict()\n",
    "\n",
    "# Function to calculate Avg_Point_Differential\n",
    "def calculate_avg_point_differential(row):\n",
    "    team1_avg_score = team_avg_score_dict.get(row['Team1'], 0)\n",
    "    team2_avg_score = team_avg_score_dict.get(row['Team2'], 0)\n",
    "    return team1_avg_score - team2_avg_score\n",
    "\n",
    "# Calculate Avg_Point_Differential for each row\n",
    "df['Avg_Point_Differential'] = df.apply(calculate_avg_point_differential, axis=1)\n",
    "\n",
    "# Normalize team names function\n",
    "def normalize_team_name(name):\n",
    "    return name.replace(\" \", \"\").lower()\n",
    "\n",
    "# Normalize and map 'Team1' names\n",
    "df['Team1'] = df['Team1'].str.replace(' ', '', regex=True).apply(normalize_team_name).map(teams_id)\n",
    "\n",
    "# Normalize and map 'Team2' names\n",
    "df['Team2'] = df['Team2'].str.replace('^(?:@|vs)\\s*', '', regex=True).apply(normalize_team_name).map(teams_id)\n",
    "\n",
    "# Convert dates to datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Extract and process scores\n",
    "score_pattern = r'(\\d{2,3})\\s*[-–]\\s*(\\d{2,3})'\n",
    "scores = df['Score'].str.extract(score_pattern)\n",
    "df['Team1_Score'] = scores.iloc[:, 0].astype(float)\n",
    "df['Team2_Score'] = scores.iloc[:, 1].astype(float)\n",
    "df['Score'] = df['Score'].str.extract('(\\d{2,3}\\s*[-–]\\s*\\d{2,3})')[0]\n",
    "\n",
    "# Optional steps for calculating additional metrics like average scores, win ratios, etc., can be performed here.\n",
    "\n",
    "# Final cleanup: remove unnecessary columns if needed\n",
    "# For example, to drop the original 'Score' column now that we have 'Team1_Score' and 'Team2_Score':\n",
    "df.drop('Score', axis=1, inplace=True)\n",
    "\n",
    "# Display the head of the DataFrame to verify changes\n",
    "print(df.head())\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "csv_file_name = 'updated_nba_data.csv'\n",
    "df.to_csv(csv_file_name, index=False)\n",
    "print(f\"Updated data saved to '{csv_file_name}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a1dd77-5510-4c0b-8eec-301b5cd1ad10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7fdcc9-0508-410f-88e8-6890674470da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6607a636-2fd8-4304-b696-10fd96edcbf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "aa192efd-3045-48bc-92a6-c8596a724adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date     Score  Win  Type  Team1  Team2  OT  Team1_Score  Team2_Score  \\\n",
      "0 2022-10-19  117- 107    1     0     -1     -1   0        117.0        107.0   \n",
      "1 2022-10-21   108- 98    1     0     -1     -1   0        108.0         98.0   \n",
      "2 2022-10-23  109 -126    0     0     -1     -1   0        109.0        126.0   \n",
      "3 2022-10-26  118- 113    1     0     -1     -1   0        118.0        113.0   \n",
      "4 2022-10-28  136- 112    1     0     -1     -1   0        136.0        112.0   \n",
      "\n",
      "   Regular_Win_Ratio  Playoff_Win_Ratio  Similarity_Score   Avg_Score  \\\n",
      "0           0.515789           0.395349          0.838710  119.275362   \n",
      "1           0.515789           0.395349          0.838710  119.275362   \n",
      "2           0.515789           0.395349          0.725806  119.275362   \n",
      "3           0.515789           0.395349          0.919355  119.275362   \n",
      "4           0.515789           0.395349          0.612903  119.275362   \n",
      "\n",
      "   Avg_Win_Ratio  Point_Differential  \n",
      "0       0.478261               -10.0  \n",
      "1       0.478261               -10.0  \n",
      "2       0.478261                17.0  \n",
      "3       0.478261                -5.0  \n",
      "4       0.478261               -24.0  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 106\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Save to CSV if requested\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m save_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDo you want to save the updated data to a CSV file? (yes/no): \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_file \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    108\u001b[0m     csv_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter the name for the CSV file (e.g., \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupdated_nba_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py:1261\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1259\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1260\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1262\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py:1304\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1302\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1303\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dictionary for mapping team names to their IDs\n",
    "teams_id = {\n",
    "    'atlantahawks': 1610612737, 'bostonceltics': 1610612738, 'clevelandcavaliers': 1610612739,\n",
    "    'neworleanspelicans': 1610612740, 'chicagobulls': 1610612741, 'dallasmavericks': 1610612742,\n",
    "    'denvernuggets': 1610612743, 'goldenstatewarriors': 1610612744, 'houstonrockets': 1610612745,\n",
    "    'losangelesclippers': 1610612746, 'losangeleslakers': 1610612747, 'miamiheat': 1610612748,\n",
    "    'milwaukeebucks': 1610612749, 'minnesotatimberwolves': 1610612750, 'brooklynnets': 1610612751,\n",
    "    'newyorkknicks': 1610612752, 'orlandomagic': 1610612753, 'indianapacers': 1610612754,\n",
    "    'philadelphia76ers': 1610612755, 'phoenixsuns': 1610612756, 'portlandtrailblazers': 1610612757,\n",
    "    'sacramentokings': 1610612758, 'sanantoniospurs': 1610612759, 'oklahomacitythunder': 1610612760,\n",
    "    'torontoraptors': 1610612761, 'utahjazz': 1610612762, 'memphisgrizzlies': 1610612763,\n",
    "    'washingtonwizards': 1610612764, 'detroitpistons': 1610612765, 'charlottehornets': 1610612766,\n",
    "}\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv('nba_data_2023_to_2024.csv')\n",
    "\n",
    "\n",
    "\n",
    "# Correct date conversion\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Extract and split score values\n",
    "score_pattern = r'(\\d{2,3})\\s*[-–]\\s*(\\d{2,3})'\n",
    "scores = df['Score'].str.extract(score_pattern)\n",
    "df['Team1_Score'] = scores.iloc[:, 0].astype(float)\n",
    "df['Team2_Score'] = scores.iloc[:, 1].astype(float)\n",
    "\n",
    "# Clean up 'Score' column to only show score values\n",
    "df['Score'] = df['Score'].str.extract('(\\d{2,3}\\s*[-–]\\s*\\d{2,3})')[0]\n",
    "\n",
    "# Calculate average scores and win ratios\n",
    "average_scores = df.groupby('Team1')['Team1_Score'].mean().rename('Avg_Score')\n",
    "win_ratios = df.groupby('Team1')['Win'].mean().rename('Avg_Win_Ratio')\n",
    "\n",
    "# Initialize dictionaries for game counts\n",
    "regular_games = {}\n",
    "playoff_games = {}\n",
    "\n",
    "# Count wins and losses\n",
    "for index, row in df.iterrows():\n",
    "    team_id = row['Team1']\n",
    "    game_type = 'playoff' if row['Type'] == 1 else 'regular'\n",
    "    win = row['Win'] == 1\n",
    "    \n",
    "    if team_id not in regular_games:\n",
    "        regular_games[team_id] = {'wins': 0, 'losses': 0}\n",
    "    if team_id not in playoff_games:\n",
    "        playoff_games[team_id] = {'wins': 0, 'losses': 0}\n",
    "    \n",
    "    if game_type == 'regular':\n",
    "        if win:\n",
    "            regular_games[team_id]['wins'] += 1\n",
    "        else:\n",
    "            regular_games[team_id]['losses'] += 1\n",
    "    elif game_type == 'playoff':\n",
    "        if win:\n",
    "            playoff_games[team_id]['wins'] += 1\n",
    "        else:\n",
    "            playoff_games[team_id]['losses'] += 1\n",
    "\n",
    "# Calculate win ratios\n",
    "regular_win_ratios = {team_id: record['wins'] / (record['wins'] + record['losses']) for team_id, record in regular_games.items() if (record['wins'] + record['losses']) > 0}\n",
    "playoff_win_ratios = {team_id: record['wins'] / (record['wins'] + record['losses']) for team_id, record in playoff_games.items() if (record['wins'] + record['losses']) > 0}\n",
    "\n",
    "# Convert to DataFrames and merge\n",
    "regular_win_ratios_df = pd.DataFrame(list(regular_win_ratios.items()), columns=['Team1', 'Regular_Win_Ratio'])\n",
    "playoff_win_ratios_df = pd.DataFrame(list(playoff_win_ratios.items()), columns=['Team1', 'Playoff_Win_Ratio'])\n",
    "df = pd.merge(df, regular_win_ratios_df, on='Team1', how='left')\n",
    "df = pd.merge(df, playoff_win_ratios_df, on='Team1', how='left')\n",
    "\n",
    "# Calculate similarity score\n",
    "df['Score_Difference'] = abs(df['Team1_Score'] - df['Team2_Score'])\n",
    "max_diff = df['Score_Difference'].max()\n",
    "df['Similarity_Score'] = 1 - (df['Score_Difference'] / max_diff)\n",
    "df.drop('Score_Difference', axis=1, inplace=True)\n",
    "\n",
    "# Join average score and win ratio\n",
    "df = df.join(average_scores, on='Team1')\n",
    "df = df.join(win_ratios, on='Team1')\n",
    "\n",
    "# Calculate Point Differential\n",
    "df['Point_Differential'] = df['Team2_Score'] - df['Team1_Score']\n",
    "\n",
    "\n",
    "# Function to normalize team names\n",
    "def normalize_team_name(name):\n",
    "   # name = name.lower().replace(\" \", \"\").lstrip('@').lstrip('vs').strip()\n",
    "    name.lower().replace(\" \", \"\")\n",
    "    return name\n",
    "\n",
    "# Normalize and map 'Team1' and 'Team2' names\n",
    "df['Team1'] = df['Team1'].apply(normalize_team_name).map(teams_id)\n",
    "df['Team2'] = df['Team2'].apply(normalize_team_name).map(teams_id)\n",
    "\n",
    "# Option 1: Fill NaN values with a placeholder (-1) before converting to int\n",
    "df['Team1'] = df['Team1'].fillna(-1).astype(int)\n",
    "df['Team2'] = df['Team2'].fillna(-1).astype(int)\n",
    "\n",
    "# Show the updated DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Save to CSV if requested\n",
    "save_file = input(\"Do you want to save the updated data to a CSV file? (yes/no): \").strip().lower()\n",
    "if save_file == 'yes':\n",
    "    csv_file_name = input(\"Enter the name for the CSV file (e.g., 'updated_nba_data.csv'): \").strip()\n",
    "    df.to_csv(csv_file_name, index=False)\n",
    "    print(f\"Updated data saved to '{csv_file_name}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cebb9a-d770-46c2-863a-f0fc0670db9c",
   "metadata": {},
   "source": [
    "# XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d4936f-900e-47e3-87bf-3bbd2fb155fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('Ready_for_XGBoost.csv')\n",
    "\n",
    "# Convert 'Date' to datetime format and sort\n",
    "#data['Date'] = pd.to_datetime(data['Date'], unit='s')\n",
    "#data.sort_values('Date', ascending=True, inplace=True)\n",
    "\n",
    "# Drop columns that might lead to data leakage\n",
    "data = data.drop(['Team1_Score', 'Team2_Score', 'Date'], axis=1)\n",
    "\n",
    "# Prepare features and target\n",
    "X = data.drop(['Win'], axis=1)\n",
    "y = data['Win']\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "categorical_features = ['Type', 'Team1', 'Team2']\n",
    "X_encoded = encoder.fit_transform(X[categorical_features])\n",
    "feature_names = encoder.get_feature_names_out(categorical_features)\n",
    "\n",
    "# Replace categorical features with their encoded versions\n",
    "X = X.drop(categorical_features, axis=1)\n",
    "X_encoded = pd.DataFrame(X_encoded, columns=feature_names)\n",
    "X = pd.concat([X.reset_index(drop=True), X_encoded], axis=1)\n",
    "\n",
    "# Splitting the dataset for the last 10 games prediction\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=20, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize XGBoost model\n",
    "model = XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False)\n",
    "\n",
    "# Hyperparameters grid\n",
    "param_grid = {\n",
    "    'max_depth': [4, 5, 6],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 0.9, 1],\n",
    "     #   'reg_alpha': [0.01, 0.1],  # L1 regularization term\n",
    " # 'reg_lambda': [1, 10]  # L2 regularization term\n",
    "}\n",
    "#param_grid = {\n",
    "  #  'max_depth': [4, 5, 6],\n",
    " #   'n_estimators': [100, 200, 300],\n",
    "  #  'learning_rate': [0.01, 0.05, 0.1],\n",
    "  #  'subsample': [0.8, 1.0],\n",
    " #   'reg_alpha': [0.01, 0.1],  # L1 regularization term\n",
    " #   'reg_lambda': [1, 10]  # L2 regularization term\n",
    "#}\n",
    "\n",
    "# Grid search for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='roc_auc', cv=10, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Validation metrics\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "print(\"Validation Metrics\")\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_val_pred)}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_val, best_model.predict_proba(X_val)[:, 1])}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_val, y_val_pred)}\")\n",
    "print(classification_report(y_val, y_val_pred, zero_division=0))  # Adjusting zero_division parameter here\n",
    "\n",
    "# Predictions on test set (last 10 games)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "print(\"\\nTest Metrics (Last 10 Games)\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_test_pred)}\")\n",
    "\n",
    "# Display predictions for the last 10 games\n",
    "#for i, (actual, predicted) in enumerate(zip(y_test, y_test_pred), start=1):\n",
    "   #print(f\"Game {i}: Actual Outcome: {'Win' if actual == 1 else 'Loss'}, Predicted Outcome: {'Win' if predicted == 1 else 'Loss'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8672d877-ecd9-46b2-b6f5-fb2fcbedb97b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43656005-cfdb-4e0b-a0f8-de93470e034c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "376c4933-0b4f-4e96-ab37-d53847b5bc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics\n",
      "Accuracy: 0.6124411704522201\n",
      "ROC AUC: 0.6495986942489\n",
      "Confusion Matrix:\n",
      "[[1400 1030]\n",
      " [ 864 1593]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.58      0.60      2430\n",
      "           1       0.61      0.65      0.63      2457\n",
      "\n",
      "    accuracy                           0.61      4887\n",
      "   macro avg       0.61      0.61      0.61      4887\n",
      "weighted avg       0.61      0.61      0.61      4887\n",
      "\n",
      "\n",
      "Test Metrics (Last 25 Games)\n",
      "Accuracy: 0.24\n",
      "ROC AUC: 0.6578947368421052\n",
      "Confusion Matrix:\n",
      "[[ 0 19]\n",
      " [ 0  6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        19\n",
      "           1       0.24      1.00      0.39         6\n",
      "\n",
      "    accuracy                           0.24        25\n",
      "   macro avg       0.12      0.50      0.19        25\n",
      "weighted avg       0.06      0.24      0.09        25\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Tommy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('Ready_for_XGBoost.csv')\n",
    "\n",
    "# Convert 'Date' to datetime format and sort\n",
    "#data['Date'] = pd.to_datetime(data['Date'], unit='s')\n",
    "#data.sort_values('Date', ascending=True, inplace=True)\n",
    "\n",
    "# Drop columns that might lead to data leakage\n",
    "data = data.drop(['Team1_Score', 'Team2_Score'], axis=1)\n",
    "\n",
    "# Prepare features and target\n",
    "X = data.drop(['Win', 'Date'], axis=1)\n",
    "y = data['Win']\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "categorical_features = ['Type', 'Team1', 'Team2']\n",
    "X_encoded = encoder.fit_transform(X[categorical_features])\n",
    "feature_names = encoder.get_feature_names_out(categorical_features)\n",
    "\n",
    "# Replace categorical features with their encoded versions\n",
    "X = X.drop(categorical_features, axis=1)\n",
    "X_encoded = pd.DataFrame(X_encoded, columns=feature_names)\n",
    "X = pd.concat([X.reset_index(drop=True), X_encoded], axis=1)\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=25, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize XGBoost model\n",
    "model = XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False)\n",
    "\n",
    "# Hyperparameters grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'reg_alpha': [0.01, 0.1],  # L1 regularization term\n",
    "    'reg_lambda': [1, 10]  # L2 regularization term\n",
    "}\n",
    "\n",
    "# Grid search for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='roc_auc', cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Validation metrics\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "print(\"Validation Metrics\")\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_val_pred)}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_val, best_model.predict_proba(X_val)[:, 1])}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_val, y_val_pred)}\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Predictions on test set\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "print(\"\\nTest Metrics (Last 25 Games)\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_test_pred)}\")\n",
    "print(classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e85f9746-fb57-47e5-8cc7-42b948c7747f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics\n",
      "Accuracy: 0.6020867430441899\n",
      "ROC AUC: 0.6344016105529796\n",
      "Confusion Matrix:\n",
      "[[1373 1048]\n",
      " [ 897 1570]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.57      0.59      2421\n",
      "           1       0.60      0.64      0.62      2467\n",
      "\n",
      "    accuracy                           0.60      4888\n",
      "   macro avg       0.60      0.60      0.60      4888\n",
      "weighted avg       0.60      0.60      0.60      4888\n",
      "\n",
      "\n",
      "Test Metrics (Last 10 Games)\n",
      "Accuracy: 0.2\n",
      "ROC AUC: 0.75\n",
      "Confusion Matrix:\n",
      "[[ 0 16]\n",
      " [ 0  4]]\n",
      "Game 1: Actual Outcome: Loss, Predicted Outcome: Win\n",
      "Game 2: Actual Outcome: Loss, Predicted Outcome: Win\n",
      "Game 3: Actual Outcome: Loss, Predicted Outcome: Win\n",
      "Game 4: Actual Outcome: Win, Predicted Outcome: Win\n",
      "Game 5: Actual Outcome: Loss, Predicted Outcome: Win\n",
      "Game 6: Actual Outcome: Loss, Predicted Outcome: Win\n",
      "Game 7: Actual Outcome: Loss, Predicted Outcome: Win\n",
      "Game 8: Actual Outcome: Loss, Predicted Outcome: Win\n",
      "Game 9: Actual Outcome: Loss, Predicted Outcome: Win\n",
      "Game 10: Actual Outcome: Loss, Predicted Outcome: Win\n",
      "Game 11: Actual Outcome: Win, Predicted Outcome: Win\n",
      "Game 12: Actual Outcome: Loss, Predicted Outcome: Win\n",
      "Game 13: Actual Outcome: Loss, Predicted Outcome: Win\n",
      "Game 14: Actual Outcome: Loss, Predicted Outcome: Win\n",
      "Game 15: Actual Outcome: Loss, Predicted Outcome: Win\n",
      "Game 16: Actual Outcome: Loss, Predicted Outcome: Win\n",
      "Game 17: Actual Outcome: Loss, Predicted Outcome: Win\n",
      "Game 18: Actual Outcome: Win, Predicted Outcome: Win\n",
      "Game 19: Actual Outcome: Win, Predicted Outcome: Win\n",
      "Game 20: Actual Outcome: Loss, Predicted Outcome: Win\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('Ready_for_XGBoost.csv')\n",
    "\n",
    "# Convert 'Date' to datetime format and sort\n",
    "#data['Date'] = pd.to_datetime(data['Date'], unit='s')\n",
    "#data.sort_values('Date', ascending=True, inplace=True)\n",
    "\n",
    "# Drop columns that might lead to data leakage\n",
    "data = data.drop(['Team1_Score', 'Team2_Score', 'Date'], axis=1)\n",
    "\n",
    "# Prepare features and target\n",
    "X = data.drop(['Win'], axis=1)\n",
    "y = data['Win']\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "categorical_features = ['Type', 'Team1', 'Team2']\n",
    "X_encoded = encoder.fit_transform(X[categorical_features])\n",
    "feature_names = encoder.get_feature_names_out(categorical_features)\n",
    "\n",
    "# Replace categorical features with their encoded versions\n",
    "X = X.drop(categorical_features, axis=1)\n",
    "X_encoded = pd.DataFrame(X_encoded, columns=feature_names)\n",
    "X = pd.concat([X.reset_index(drop=True), X_encoded], axis=1)\n",
    "\n",
    "# Splitting the dataset for the last 10 games prediction\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=20, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize XGBoost model\n",
    "model = XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False)\n",
    "\n",
    "# Hyperparameters grid\n",
    "param_grid = {\n",
    "    'max_depth': [4, 5, 6],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 0.9, 1],\n",
    "     #   'reg_alpha': [0.01, 0.1],  # L1 regularization term\n",
    " # 'reg_lambda': [1, 10]  # L2 regularization term\n",
    "}\n",
    "#param_grid = {\n",
    "  #  'max_depth': [4, 5, 6],\n",
    " #   'n_estimators': [100, 200, 300],\n",
    "  #  'learning_rate': [0.01, 0.05, 0.1],\n",
    "  #  'subsample': [0.8, 1.0],\n",
    " #   'reg_alpha': [0.01, 0.1],  # L1 regularization term\n",
    " #   'reg_lambda': [1, 10]  # L2 regularization term\n",
    "#}\n",
    "\n",
    "# Grid search for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='roc_auc', cv=10, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Validation metrics\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "print(\"Validation Metrics\")\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_val_pred)}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_val, best_model.predict_proba(X_val)[:, 1])}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_val, y_val_pred)}\")\n",
    "print(classification_report(y_val, y_val_pred, zero_division=0))  # Adjusting zero_division parameter here\n",
    "\n",
    "# Predictions on test set (last 10 games)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "print(\"\\nTest Metrics (Last 10 Games)\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_test_pred)}\")\n",
    "#print(classification_report(y_test, y_test_pred, zero_division=0))  # Adjusting zero_division parameter here\n",
    "\n",
    "# Display predictions for the last 10 games\n",
    "for i, (actual, predicted) in enumerate(zip(y_test, y_test_pred), start=1):\n",
    "   print(f\"Game {i}: Actual Outcome: {'Win' if actual == 1 else 'Loss'}, Predicted Outcome: {'Win' if predicted == 1 else 'Loss'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948ea136-acef-4b2b-8af8-ae54d2ce8637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc081e4-160c-46c4-90f6-96d58bab998a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ad67a1c-84d1-4afc-b686-5deaa2620a96",
   "metadata": {},
   "source": [
    "# Updated CSV, new futures added. Model adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db25ce8c-1719-43fa-946a-d27fd3fc917f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278a35ab-1e76-4b00-966f-efe4f69a46a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
